                                                                   Amazon Web Services / July 25, 24
                                                         =================================================

Course Content
==================

1) Challenges with on-prem infrastructure

2) What is Cloud Computing and Why ?

3) Cloud services models (IaaS vs PaaS vs SaaS)

4) AWS introduction

5) AWS free tire account setup

6) Region & availability zones

7) AWS service Tour

8) EC2 (Elastic Compute Cloud) -> Virtual servers

 
    - AMI
    - Instance Type
    - keypair
    - Security Group
    - EBS (Volumes & Snapshots)
    - Load Balancer
    - Auto Scaling

9) S3 (Simple Storage Service) -> Unlimited storage

10) RDS (Relational Database Service) -> Oracle/MySQL/Postgres

11) DynamoDB (NoSQL Database)

12) IAM (Identity & Access Management)

13) VPC (Virtual Private Cloud) -> Network

   - Types of IPs
   - VPC Sizing
   - Subnets
   - Route Tables
   - IGW
   - Nat
   - Peering
   - NACLs

14) Cloud Watch -> Monitoring Resource

15) SNS -> Simple Notification Service

16) EBS -> Elastic Bean Stack -> PaaS

17) AWS Lambdas -> Serverless Computing

18) EFS -> Elastic File System

19) ECS -> Elastic Container service - Docker Container

20) EKS -> Elastic Kubernetes Service

21) Cloud Front -> CDN

22) Route 53 -> DNS Mapping

23) AWS CLI -> Command line Interface

24) Billing Overview


---

                                                                26-July-24
                                                        ==========================



 IT Infrastructure
=====================

a) Machines
b) Network
c) Power
d) Storage
e) backup
f) Security

=> Infrastructure is of 2 types

  1) On-prem Infrastructure
  2) Cloud Infrastructure

===========================
On-Prem  Infrastructure
==========================

=> On premis means we need to purchase and we need to maintain our resources to run our business.

=> We have several challenges with On-prem infrastructure

   1) Lot of investment
   2) Lot of man power
   3) Scalability   (Increase/ Decrease)
   4) Availability
   5) Disaster recovery


=> To overcome the challenges of on-prem infrastructure, companies are preferring cloud infrastructure.


=============================
What is Cloud Computing
=============================

=> The process of delivering IT resources over the internet on demand basis is called Cloud Computing.

=> We have below  advantages with the cloud computing.

   - Pay as you go
   - Less cost
   - Scalability
   - Availability
   - Security
   - Backup


=====================
Cloud Providers
=====================

=> The companies which are providing IT infrastructure based on pay as you go model is called as Cloud Providers

  - Amazon (AWS)
  - Microsoft (Azure)
  - Google (GCP)
  - Salesforce
  - Ali Baba
  - Digital Ocean


=========================
Cloud Service Models
=========================

1) IAAS

2) PAAS

3) SAAS


What is Iaas
=================

=> Iaas stands for infrastructure as a service

=> Provider will give infrastructure for us

   Ex : Machines, network, storage

=> As a customer we need to prepare platform to run our application

   Ex: install required softwares + Setup web server + deploy application


What is PaaS
================

=> PaaS stands for platform as a service

=> Provider will give ready made platform to run our application directly

=> As a customer we need to take care of only application  deployments.

What is SaaS
=================

=> SaaS stands for software as a service

=> Cloud provider will give their to run our business.

  Ex:  Zoom, Google Drive, Dropbox, Jira.....


AWS Cloud
=============

=> AWS stands for Amazon web services

=> AWS providing cloud services from 2006

=> AWS works on pay as you go model

=> 190+ countries using AWS cloud services to run their business.

=> AWS having global infrastructure.

     33 Regions
     105 availability zones.


=======================
How to use AWS cloud
=======================

=> We can create free tier account in AWS for practice

Note ; If we use any paid service, bill will be generated. AWS will not deduct  bill amount from the from the card directly.

=> AWS will send reminder for bill payment. If we dint pay the bill,  then our AWS account will be suspended.

Note : We can request AWS support team to make bill amount as zero for 1 or 2 times.


===================
AWS Services
===================


=> AWS has 200+ services
 
1) EC2 : to create virtual machines (Hourly Billing)

2) S3 : Unlimited storage

3) RDS : Relational Database Service

4) EFS : Elastic File System

5) IAM : Identity & Access management

6) VPS : Virtual Private Cloud

7) Elastic Beanstalk : End to end web application mgmt. (PaaS)

8) Lambdas : Serverless computing

9) Route 53 : Domain Mapping (DNS)

10) ECS : Elastic Container Service

11) EKS : Elastic Kubernetes Service

12) Cloud Watch : Monitoring

13) SNS : Simple Notification Service



---

                                                                27-July-24 || Session 03
                                                        =======================================

Elastic Compute Cloud (EC2)
=============================

=> Its Most Demanded service in AWS

=> EC2 VM is knows as EC2 instance

   EC2 instance = Computer / server / VM / Virtual machine / V Box..

=> EC2 instance is resizable (We can change configuration based on demand)

=> EC2 is paid service

=> EC2 VM Minimum billing period is 1 hour

Note : To encourage beginners AWS giving t2.micro/t3.micro for 1 year free.

=> EC2 VM will have storage with EBS service.

  EC2 VM with Windows OS : 30 GB Default
 
  EC2 VM with Linex OS : 8 GB Defauld

  EBS Max capacity : 16 TB

=> For EC2 instances network will be provided by VPC service.

=> For EC2 VM creation we will use AMI (Amazon Machine Image)


Note : AMI represents configuration required for the machine

  - Windows AMI
  - Amazon Linex AMI
  - Ubuntu AMI
  - redHAT AMI

=> To secure EC2 VM we will use key-pair (public key and private key)

=> To allow incoming and Out going traffic  for EC2 we will use security group

Note : One key pai we can use it for multiple EC2 instances
 
       One security group(SG) we can use for multiple EC2 instances


===========================
Lab Particles on EC2 VM
===========================

1) Create Key-pair (.Pem file)

  - Public Key (AWS will keep it)
  - Private key (We will get it)

2) Create security group and update inbound rules to allow traffic

  windows RDP : 3389
  Linex SSH : 22
  HTTP : 80
  HTTPS : 443
  MySQL : 3306

3) Create EC2 Instance

  - Select AMI
  - Select Instance Type
  - Select Key-pair
  - Select Security Group
  - Select EBS volume
  - Launch Instance
 
4) Connect  to EC2 instance using RDP client.


 
---

                                                                29-July-24 || Session 04
 
                                                      =======================================

Types id IP's in AWS cloud
=============================

=> We have 3 types of IP's in AWS cloud

 1) Private IP
 2) Public IP
 3) Elastic IP

=> Private IP is a fixed IP in AWS. Its used for internal communication (with in VPC)

=> Public IP is dynamic IP in AWS. Its used to connect with EC2 VM from outside.

=> When VM is restart, then public IP changes.

=> If we want fixed public IP then we need to use public Ip

=> Elastic IPs are commercial (Bill will be generated)


==================================
Lab Practicals on elastic IP
==================================

Step 1 : Allocate elastic IP (getting from AWS)
     2 : Associate elastic IP with EC2 VM
     3 : Restart VM and check public IP (it will not Change)
     4 : De-associate elastic IP
     5 : Release elastic IP to AWS (to avoid billing)


=================
What is EBS
=================

=> EBS stands for Elastic Block Store.

=> Its block level storage device (Hard Dick / SSD)

=>  When we create EC2 instance then EBS volume gets created automatically.

Note: If we remove EBS volume from  EC2 instance, then we cont start/use that EC2 instance.


=> IN EBS we have 2 types of volumes.
 
   1) Root volume
   2) Additional volume.

Note : When we create an instance, by default we will get one root volume.

=> Root volume is mandatory to launce EC2 instance.

=> If we remove root volume from EC2, then we cont use/ start that instance.

=> Additional volumes are optional devices (we can add/ remove)

=> For windows VM, we will get 30 GB as default volume size.

=> For Linex VM, we will get 8 GB as default volume size.

NOte : EBS volume can have upto 16 TB .

=> One VN can have multiple EBS volumes

=> One EBS volume can be attached to one EC2 VM at a time.

=> EBS volumes are availability on zone specific.


   Mumbai : ap-south-1
            ap-south-1a
            ap-south-1b
            ap-south-1c

---

                                                                30-July-24 || Session 05
 
                                                      =======================================


EBS Volume Types
====================

=> We have 5 types of volumes

1) general purpose volume (Min 1 GiB / Max 16384 GiB)

2) Provisioned IPOS (io1) (Min 4 GiB / Max 16384 GiB)

                     (io2) => Min 4 GiB , Max 65536 Gib

3) Cold HDD (Min 128 GiB / Max 17384 GiB)

4) Thruput optimized (Min 128 GiB / Max 16384 GiB)

5) Magnetic standard (Min 1 GiB / Max 1024 GiB)



===========================
Lab Task on EBS Volumes
===========================

1) Create EC2 VM (VM-1) (Amazon Linex AMI) - we will get EBS root vol 8 GB

2) Create additional EBS vol with 10 GB (check AZ)

3) Attach additional volume to EC2 VM-1

Note : Now EC2 VM-1 will have 2 EBS volumes (Root volume + additional volume)

4) connect with VM-1 and check volumes attached

  $ 1SBLK

5) Store the data in additional volume using mounting with below command
 
    sudo mkfs -t ext4 /dev/xvdb
    mkdir ashokit
    sudo mount /dev/xvdb/ashokit
    cd ashokit
    sudo touch f1.txt f2.txt

Note : path of additional volume :: /dev/xvdb


6) Detach additional volume from EC2 VM-1

7) Create new EC2 VM (VM-2) and attach EBS additional volume to EC2 VM-2

8) Check the data present or no

    lsblk
    mkdir demo
    sudo mount /dev/xvdb  demo
    ls -l demo

Note : Once practice is completed, detach additional volume and delete it to avoid billing

Note : To check EC2 VM connected volumes
 
   $  lsblk

 

What is MOunting ?
==================

Establishing a link between additional volume and the directory is called Mounting.

Step 1 : create a directory in the VM machine.

  mkdir ashokit

step 2 : establish link between additional volume and VM directory

   sudo mount /dev/xvdb ashokit

step 3 : direst to ashokit directory
 
      cd ashokit

step : 4 create files f1, f2

   sudo touch f1.txt f2.txt



---

$lsblk

[ec2-user@ip-172-31-13-53 ~]$ lsblk
NAME      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
xvda      202:0    0    8G  0 disk
├─xvda1   202:1    0    8G  0 part /
├─xvda127 259:0    0    1M  0 part
└─xvda128 259:1    0   10M  0 part /boot/efi  ---> Additional volume
xvdb      202:16   0  100G  0 disk
[ec2-user@ip-172-31-13-53 ~]$


---

Snapshots
=================

=> Snapshots are used for volume backup

  - Snapshots are region specific
  - volumes are AZ specific

=> From volume we can create snapshot and from snapshot we can create volume.

   volume ---> snapshot ----> volume

=> Snapshot cannot be connect to EC2 instance directly



---

How to copy data from 1a VM zone to 1b Vm zone ?
=================================================

step 1 : Create snapshot for 1a VM volume

step 2 : From snapshot create volume in 1b zone

step3 : Attached newly created 1b volume to 1b VM


Assignment
===============

How to recover EC2 VM when we lost pem file


===========================================================================================================================================================================================

                                                                30-July-24 || Session 05
 
                                                         =======================================

1) What is userdata

2) Hosting static website

3) Load balancer

4) OSI Model


What is user data in EC2 VM
==================================

- Its used to execute script while launching EC2 VM. (So we dont need to connect to the machine to execute the script)

- User data will execute only once

- Create EC2 VM with below user data.


#!/bin/bash
   sudo su
   yum install httpd -y
   cd /var/www/html
   echo "<html><h1>Life insurance Server -1</h1></html" >index.html
   service httpd start


Note : Once EC2 VM started enable http protocol in EC2 VM security group inbound rule.

- Access static website using public IP



What is Load Balancer
========================

=> When we run our application in single server, there are challenger below.

1) One server should handle all requests
2) Burden will increase on server
3) Response is delayed for clients
4) Server can crash
5) Single point of failure
6) Business loss

=> To avoid above problems, we will run our application using load balancer.

=> Load balancer is used to distribute the load to multiple server in round robbin fashion.

=> We have below advantages with load balancer.

- App will run in multiple servers.

- Load will  be distributed.

- Burden will be reduced on servers

- fast performance

- High availability


=> In AWS we have 4 types of load balancer

 1) Application load balancer (ALB)
 2) Network load balancer (NLB)
 3) gateway load balancer(GLB)
 4) Classic load balancer (Pervious generator)



======================================
Load balancer lab task
======================================

STEP 1 : Craete EC2 VM-1 VM with below given data

#! /bin/bash

   sudo su
   yum install httpd -y
   cd /var/www/html
   echo "<html><h1>Life insurance Server -1</h1></html">index.html
   service httpd start


STEP 2 : Craete EC2 VM-2 with below given data

#! /bin/bash

   sudo su
   yum install httpd -y
   cd /var/www/html
   echo "<html><h1>Life insurance Server -2</h1></html" >index.html
   service httpd start

STEP 3  ; Add these 2 instances to one "Target Group"

STEP 4 : Create load balancer with Targer group (ALB)

     scheme : Internet facing

Note : ALB runs on Http protocol with 80 port number

STEP 5 : Access load balancer DNS in browser

(It will route the requests to our servers in round robbin fashion)

Note : After practice delete the load balancer to avoid billing.



Assignment : How to implement Load balancer with multiple target groups

https://www.youtube.com/watch?v=QvEJ8--zneU ###


====================================
Monolithic VS Microservices
====================================

- If we develop all the functionalities in single app, then it is called as monolithic applicxation

note : We can use single target group when we have monolithic 	application

- If we develop all the functionalities using different APIs then its called as microservices architecture.

Note : we need to use multiple target groups when we have microservice application.


============================
Auto Scaling
========================

=> Its used to adjust the capacity required to handle the load of our application

Ex : For big billion day sale we con not guess traffic.

=> If requests are increasing then servers should be increased and if the requests are decreasing then servers should be reduced.

=> we have below advantages with  auto scaling.

1) Cost management

2) High Availability

3) Fault Tolerance


=> To increase Auto Scaling group. We will use launch template.

=> Launch template is used to specify configuration required to launch new VM.



===========================================================================================================================================================================================

                                                                         05-Aug-24
                                                                       ==============

Fault Tolerance
==================
When a VM is deleted or damaged, the auto scaling group will create an instance. this is called fault tolerance.


Assignment on AMI
======================
1) Create Linux VM ( VM1)
2) Host static website in Linux VM1
3) Create AMI from Linux VM1 (AMI name : ashokit AMI)
4) Create new Linux VM (VM 2) using ashokit AMI
5) Verify static website working in new Linux VM (VM2)


===========================================================================================================================================================================================

                                                                         06-Aug-24
                                                                       ==============
1) What is EC2 ? and  why is EC2 ?

2) What is AMI and how to create AMI

3) What is keypair ?

4) What is security group ?

5) What is EBS ? (Volumes and snapshots)

6) Launching windows VM & RDP

7) Launching Linux VM & SSH

8) Static website hosting in EC2

9) Load Balancer

10) Target Group

11) OSI models

12) Monolithic VS Microservices

13) Launch template

14) Auto Scaling

15) Types of IPs (Its used to connect to our machines from outside of the AWS)



Interview Question : What are the EC2 instance types available.

========================================================================================================================

EC2 Instance type
========================

=> Amazon EC2 (Elastic Compute Cloud) offers a variety of instance types to suite different use cases.

1) general purpose
 
    - T series : t2, t3, t4 ..
 
    - M series : m6g, m5, m5a, m5n ..

2) Compute Optimized

    - C series : c7mg, c6i, c6g, c5, c5a

3) Memory Optimized

   - R series : r6g, r5, r5a, r5n

4) Storage Optimized

  - I series : i4i, i3, i3en

  - D series : d2

5) Accelerated Computing

  - P series : p4, p3, p2

  - G series : g5, g4ad, g4dn
 
  - inf series : inf1

6) High Performance Computing

  - H series : hpc6id

=============================================================================================

EC2 instances have below categories
========================================

a) On demand instances (no commitment)

b) Reserved Instances (Booking for 1 or 3 years)

c) Spot request (bidding)

d) Dedicated Host (physical machines)


=====================================================
What is OSI  model  (Open System Interconnection)

======================================================

=> Open system Interconnection

=> OSI Model represents how request will transfer from client to server

=> OSI model contains 7 layers

layer 1 :  Physical layer

layer 2 : data link layer

layer 3 : Network layer

layer 4 : Transport layer

layer 5 : Session layer

layer 6 : Presentation layer

layer 7 : Application layer


Physical Layer (L1) : Deals with the physical connections between devices. It defines the hardware elements like cables, switches, and the electrical/optional signals used for data transmission.

Data Link Layer (L2) : Provides node - to - node data transfer. Its responsible for physical addressing of devices.

Network layer (L3) : Manages routing of data packets between devices.

Transport Layer (L4) : Ensures reliable data transfer between devices

Session layer (L5) : Manages session between application (Connection)

Presentation Layer (L6) : Handles data packet translation. It ensures data is in reliable format.

Application Layer (L7) : Provides network services directly to end user to access application


=> Application Load balancer (ALB) will work at layer-7 (Application Layer)
 
    ex : http, https protocols
         advanced routing
         header based routing
         path based routing

=> Network Load balancer (NLB) will work at layer-4 (Transport Layer)

  ex : TCP, UDP
       for gaming server
       for video streaming

=> Gateway Load balancer (GLB) will work at layer-3 (Network Layer)



                    Browser(Client) --> A --> P --> S --> T --> N --> D --> P --> App Sever

 =================
Summery
==============

===========================================================================================================================================================================================

                                                                         07-Aug-24 AWS RDS
                                                                       ========================

1) What is database

2) Why we need Database

3) On-prem database server

4) What is RDS and why ?

5) RDS setup

6) Connection with RDS DB


What is database
=====================

database : its a software used to store the data permanently.

                                SQL
  WhatsApp application -----------------------> database

                  - text messages
                  - Audio Files
                  - Video files
                  - Documents
 
=> every S/q application will use database to store the data.

=> Software application will use SQl to communicate with database.

=> SQL stands for Structured Query Language.

=> Using SQL we can perform CRUD operations in the database.

     C
     R
     U
     D

=> We have several databases in market.

         - Oracle
         - MuSQL
         - SQLServer
         - PostGres
=> the above databases are called Relational databases


====================================
Challenges with On-prem databases
====================================
1) Purchase DB server Liscence

2) Install DB server Software

3) Security

4) Network

5) Availability

6) Scalability

7) backup

8) Administration (DBA)

=> To overcome above challenges, Its highly recommended to use Cloud databases.

=> If we use cloud databases, then cloud provider will manage database server for us.

=> AWS RDS service providing cloud database

=> RDS stands for relational database service in AWS cloud.

=> RDS is used to create & manage relational databases.

=> RDS is a fully manages services in AWS cloud.

=> RDS works based on " Pay as you go" model.


====================
RDS Lab task
====================

Steps
---

1) Create MySQL Db server using RDS.

2) Enable MySQL : 3306 port number in security group inbound rules

3) Test MySQL DB connection using MySQL workbench software (Client s/w)

4) Execute some SQL queries for practice.

5) Delete RDS instance to avoid billing.


===============================
MySQL DB Creation steps
===============================

Creation method : standard create

Engine Type : MySQL

Templates : free tire

DB Instance identifies : ashokit-db-instance

public access : Yes

database options : initial DB name : sbidb

Database details : endpoint, username, password, port


Note : Using above details we can check database connectivity

=> Once connectivity is successful then we share database details with development team.

Master : username : admin

Master password : vamshi123

endpoint : ashokit-db-instance.c7i6iokcspb3.us-east-2.rds.amazonaws.com


==============
SQL queries
===============

1) create table
---

CREATE TABLE emp (
    eid INT,
    ename VARCHAR(100),
    esal INT
);


2) Insert query
---

insert into emp values(1, 'John', 1000);
insert into emp values(2, 'Gopal', 2000);


3 fetch
---

select * from emp;


Note : After practice delete RDS to avoid billing.




                              SQL developer -----------------------> Oracle DB server
 
                             MySQL work Bench ---------------------> MySQL DB server

                            Pg Admin ------------------------------> PostGre DB server


================
Assignment
===============

1) Connect with RDS DB server using EC2 linex VM and execute above SQL queries.

2) Create database in cloud and connect it with spring boot application and test it from the postman.


Spring Boot with RDS Connection : https://www.youtube.com/watch?v=GSu1g9jvFhY



===========================================================================================================================================================================================

                                                                         08-Aug-24 AWS RDS
                                                                       ========================

===================================
AWS S3 (Simple Storage Services)
===================================

=> S3 is a storage service in AWS cloud

-=> S3 supports unlimited storage

=> Using S3 we can store any amount of data from anywhere at any point of time

=> S3 supports object based storage (Files)

=> We can upload and download objects (files) at any point of time using s3
 
                    One object = one file.

=> In S3 we need to create buckets to store data.

Note : in one bucket we can store group of objects

Note : every bucket should have unique name.

=> When we create a bucket, end point URL will be generated to access bucket.

=> When we upload object into bucket, every object will get its own endpoint URL.

Note : By default objects and buckets are private (We can make them private )

=> We  can create multiple buckets in S3.


=======================================
Static website hosting using S3
=======================================

Note : S3 supports hosting static website directly. no need to create VM and deploy the web page.

=> website is collection of web pages

 Ex: Login page, register page, services, about us....

=> Websites are 2 types

  1) Static website
  2) Dynamic website

static website : The website that gives same responses for all the users

Dynamic website : The website that gives response/ content based on the user.


Steps
=======

1) Create S3 bucket with unique name.

2) Upload website files and folders into bucket with public read access.

3) Enable static website hosting (In bucket properties)

   index.html
   error.html

Note : After enabling static website hosting it generates end-point URLs for out website.

4) Access our website using website endpoint url.


=> Hosting static website


=====================
S3 storage classes
=====================

=> Storage classes are used to specify how frequently we want to access our objects from S3

=> We have several storage classes in S3

 1) Standard (default) : To access object more than once in a month.

 2) Intelligent - Tiering : Unknown access patterns

 3) Standard IA : Infrequent access based (Only once in a month)

 4) One Zone - IA: Stored in single availability zone.

 5) Glacier Instance Retrieval : Long live archive data (Once in quarter -> milli sec)

 6) Glacier flexible retrieval : Once in a year (Minutes to hours)

 7) Glacier Deep archived : Less than once in a year (Hours to download)

 8) Reduced Redundancy : Not cost effective (Not recommended)


======================
Versioning
======================

=> Its used to maintain multiple variants of same file

=> By default versioning will be disables for S3 buckets.

=> As versioning is disabled, when we upload file again it will override old file.

=> If we dont want to replace old objects from bucket, then we can enable versioning.

=> Versioning we will enable at bucket level and its applicable at object level.

Note : Once we enable versioning we cannot disable that but we can suspend it.


===========================================================================================================================================================================================

                                                                         09-Aug-24
                                                                   ========================


=======================
Object Locking
=====================

=> its used to enable the feature worn (write once read many times ) model

=> We can enable object lock on versioning enables buckets.

=> Object lock will be enabled at bucket level and its application to object level.

Note : If we enable Object locking then by default versioning will be enabled


==================================
What is Transfer Acceleration
==================================

=> Its used to speed up data transfer process in S3 bucket.

=> When we enable transfer acceleration it provides end point url to upload the data to S3 bucket quickly.
 
Note : If we enable transfer acceleration bill will be generated.



==================================
Real time usecases of S3
===================================

1) Application  files (Images, audios, videos, docs etc..)

2) DB backup files

3) EBS volumes snapshots

4) Server log files.


====================
AWS S3 Limits
====================

=> Soft limit is 100 S3 buckets per AWS account. we can request AWS support team to increase the buckets limit.

=> Individual object size can upto 5TB. for uploading large objects we can use multipart upload

Note:  In one bucket we can upload unlimited objects.


=============================================================================================
Ashok Sir GitHub Repo : gitHub.com/ashokitschool/springboot_s3_app


=> IAM user management concept



How to connect
==============


                    install MySQL Client in EC2
         EC2 -------------------------------------> RDS

                             AWS CLI
         EC2 -------------------------------------> S3 bucket



Summery
==============

1) What is S3 and why

2) S3 buckets and objects

3) Static website hosting using S3

4) S3 storage classes

5) Versioning

6) Object Locking

7) Transfer acceleration

8) S3 Limits

9) S3 usecases



Domain mapping : https://youtu.be/ruxNLQWtyCw?si=GQhOObSfqs4CxPEo



===========================================================================================================================================================================================

                                                                         12-Aug-24 IAM
                                                                   ========================

=======================================
IAM (Identity and Access Management)
=======================================

=> Its used to manage 	users, access policies and groups.

=> I am is free service.

=> In AWS cloud platform we will have 2 types of accounts

    1) Root User

    2) IAM user

Note : When we sighup in AWS website then by default it will consider that as root account.

=> Root account is very power full account with no restrictions.

=> If we login with root user credentials, we can access everything i AWS cloud.

Note : We should not use root account for day to day activities.

Note : We should not share root account credentials with anyone.

Note : Company will not provide root account credentials for team members.

Note : Its recommended to enable MFA for root account.


 

 
              AWS Console -----------AWS Cloud ---------------------(AWS CLI, Terraform, AWS SDK) Programmatic access.



Question: How AWS will store roles and policies ?

It stores in JSN format.




===========================================================================================================================================================================================

                                                                         19-Aug-24 IAM
                                                                   ========================

 
===================
IAM Account
==================

1) create I am account and attach policies (RDSFullAccess, ....)

2) Login into I am account and EC2 service (cant access because no permission)

====================
Iam user group
====================

1) Create user group

2) Attach policies to group

3) Add users to group

=================
I am Roles
=================

=> I am role is nothing but set of permissions.

=> EC2 VM wants to create EKS cluster. Then EC2 VM should have IAM role with EKS permiosssions.




===============
IAM Summery
===============
1) What is IAM
2) What is root account
3) How to enable MFA
4) What is IAM account
5) Console Access VS Programmatic Access
6) Users creation
7) Users / Groups
8) Polices / Permissions
9) Roles
10) Working with custom policies



===========================================================================================================================================================================================

                                                                        20-Aug-24 IAM
                                                                   ========================


===========
AWS CLI
=======

=> AWS provides 2 ways of infrastructure configuration

1) AWS Management web console

2) AWS CLI


============================
Using the AWS web console
============================

=> Its a graphical method to connect to various AWS resources, their configuration, modification, etc. Its simple to use and does not require knowledge of scripting


==============================
AWS Command Line Interface
=============================

Usually the script provides you with the flexibility to manage multiple AWS resources, infrastructure effectively.

=> AWS provides multiple ways to configure and manage infrastructure.

1) AWS management web console
===============================
=> Its a graphical user interface to connect with various AWS resources, their configuration, modification etc.

=> Its simple to use and does not require knowledge of scripting.


2) AWS CLI
3) Cloud Formation
4) Terraform


=================================
Configuring AWS CLI
==============================

1) Create AWS account

2) Login into AWS account & generate key

  

3) Download and install AWS CLI

 download link: https://awscli.amazonaws.com/AWSCLIV2.msi

4) Once installation completed execute below commands.

  - aws --version

  - aws configuration

Note : AWS configure command will ask for access key, secret access key, region and output format.

---




---
interview question : which aws cli version you used ?   Answer : 2.26.1

 

CLI Documentation : https://docs.aws.amazon.com/cli/latest/reference

Ex:

aws s3 ls

C:\Users\Vamshi Ellandula>aws s3 mb s3://vamshi701550


============================================
Working with AWS S3 service using CLI
============================================

=> AWS S3 is an object storage service

# List down all the buckets available
 aws S3 ls

#Create bucket in S3
 aws s3 mb s3://vamshi867623

#Copy files into bucket
 aws s3 cp text,txt s3://vamshi867623/text.tx

#Remove file from bucket
 aws s3 rm s3://vamshi867623/text.tx

#Remove bucket
 aws s3 rb s3://vamshi867623


======================================
Working with EC2 AWS CLI
======================================

# List all the EC2 instances
 aws ec2 describe-instances

# create key pair in EC2
 aws ec2 create-key-pair --key-name test --output text > test.pem

#Launch EC2 instance
aws ec2 run-instances --image-id ami-0100e595e1cc1ff7f --instance-type t2.micro --key-name test


ami id :
Note : When the EC2 instance is created, the instance id also gets created.

instance id : 
#Stop EC2 instance

#Terminate ec2 instance


======================================
Working with RDS AWS CLI
======================================

# Create Database instance

aws rds create-db-instance \ --db-instance-identifier test-mysql-instance \ --db-instance-class db.t3.micro \ --engine mysql \ --master-username admin \ --master-user-password secret99 \ --allocated-storage 20

# Delete database instance


Assignment
===========

Create below IAM resources using AWS CLI

1) Use
2) User Group
3) Policy
4) Roles

============================================
Installing AWS CLI in Ubuntu Linux machine
============================================

$ sudo 	apt-get install -y python-dev python-pyp

$ sudo pip install awscli

$aws --version

$aws configure


===========================================================================================================================================================================================

                                                                        21-Aug-24 IAM
                                                                   ========================

===============
Cloud watch
===============

=> Its used to monitoring AWS resources

=> Cloud monitoring real time monitoring of AWS resource such as Amazon Elastic Compute Cloud (EC2) instances, Amazon Elastic Block Store (EBS), volumes, Elastic load balancing and Amazon Relational Database services (RDS) instances.

==================
Use case
===============

=> Create EC2 instance in aws cloud.

=> Monitor that  EC2 instances 24/7

=> If EC2 instance CPU utilization is above 10% then trigger alarm.

=> send email notification when alarm got triggered.


                                                                              publish msg to sns                        publish
 (EC2 VM)-------> Cloud Watch ----------> Alarm trigger Condition(CPU>=20)---------------------------> SNS Topic -------------------------> subscribe to SNS topic(email.com)


===============================================
How to send email notification in AWS
===============================================

=> In AWS we have SNS to send notification

 SNS : Simple notification service


=================================
Cloud Watch & SNS Lab Task
==================================

1) Create SNS topic with email notification

2) Configure email subscription in SNS topic (you will get email )

3) Create EC2 instance and configure Cloud Watch alarm to monitor

  -< Select EC2 isntance -> action -> MOniter and troubleshoot -> Manager Cloud watch alarm -> Create Cloud watch alarm.

    Alarm Notification : Select SNS topic which we have created
 
    Alarm theshhold Avg CPU >= 5%



4) Connect to EC2 VM and increate load


sudo yum isntall stress -y

sudo stress --cpu 8 -v --timeout 180s

   Then execute stress command 3 to 4 times

5) Observe the behavious of cloud watch / Alarm / SNS (We should get email notification)

Note : When Alarm gotbbtriggered its status will be changed to "IN Alarm"

=> We can moniter our alarm history (How many times it got triggered)

  (Go to Cloud watch -> Select alarms ---> Click alarm, --> Click on history)


-> Execute top command to view avg CPU unitlization percentage



============================
Elastic Beanstack
===========================

=> Its and end-to-end web application management service

=> It provides platform as a service

=> It provides readymade platform to run our application, AWS will take care of infrastructure and platform top run your application.


dynamic web application deployment process
===================================================

1) Create network (VPC)

2) Create security group

3) Enable inbound rules

4) Create EC2 instance

5) Install required software to run the code.

 	Ex: Java, Tomcat, IIS

6) Create load balancer

7) Setup auto scaling group for high availability

8) deploy our application


Note : When we use AWS elastic bean stack services, then first 7 steps will be taken  care by Elastic Beanstcak. We are responsible for only application deployment.


Elastic beanstalk pricing model
===================================

=> There is no additional charge for elastic beanstalk.

=> We need to pay the amount for the resources which are created by bean stack.

   Ex: EC2 instance , S3 buckets, LBR ASG etc....



===========================================================================================================================================================================================

                                                                        22-Aug-24 IAM
                                                                   ========================


===============================
lab Task on elastic beanstalk
=================================

1) Create IAM role with below policies

   	AWS ElasticBeanStal kMulti ContainerDOcker
 
        AWSElastic BeanStalkWebtier

        AWSElasticBeanStalkWorkerTier

Ex Role Name : ashokbean_beanstalk_role

2) Create application using beanstalk

3) Create the environment for the application by choosing required runtime.

   java , python, .net

Note : Once environment in created it will generate DNS to access our application.

==========================================
uploading java spring boot application
==========================================



=> Take jar file of Java spring boot web application

=> Go to elastic bean stalk environment and upload your jar file and give version number for your application.

 Ex : v1.0

=> Go to environment and set SERVER_PORT =5000

 - Select environment

 - Go to configuration

 - Edit "update, logging and monitoring " option

 - Set environment property and apply.

SERVER_PORT = 5000

=> After got restarted, we can access our application by suing DNS url.

=>


===========================================================================================================================================================================================

                                                                        23-Aug-24 IAM
                                                                   ========================

=================
AWS Lambdas
=================

=> AWS lambdas are used to achieve server less computing.

=> Serverless computing means running the application without thinking about the servers

=> AWS will take care of servers required to run our application.

=> The main advantage of server less computing is it works based on 'pay as you use model'

=> If your application code is executed then only bill will be generated. If nobody accessing your application, then no bill

=> Code executed for only 5 mints : bill will be generated for only 5 minutes.




Note : need some Lambda dependencies to work our java program with AWS lambdas


======================================
Running Java Code with AWS Lambdas
======================================

1) Create Lambda Function with java 21 runtime

   - Enable functional URL
   - Auth type None (Public access)

Note : Once lambda function got created we can see URL to access that function

2) Access lamnda function using it URL


3) Upload jar  file in 'Code Source'

4) Configure Handler in runtime

     Class name : in.ashokit.Lamdahandkler

    Method name : hanglerRequest

    Handler Syntax : classname :: MethodName

Ex : in.ashokit.LambdaHandler:: handlerRequest


====================================
Static Website deployment
====================================

Approach 1) S3 static website hosting

approach 2) Take EC2 VM + install HTTPD + Run Static website

=====================================
Dynamic Website deployement
=====================================

Approach 1)Take EC2 VM then install required software and run your application

Approach 2) Elastic beanstalk (Paas)

Approach 3) Lambdas (Serverless computing)




===========================================================================================================================================================================================

                                                                        28-Aug-24 IAM
                                                                   ========================



####################################
VPC : Virtual Private Cloud
####################################

=> VPC stands for Virtual Private Cloud

=> VPC will provide Virtual network for the resources in the AWS cloud

=> Using VPC we can define our own IP addresses, subnets, route tables, internet gateways, NAT gateways.

=> VPC will provide flexible an secure network environment that enables users to manage their cloud resources.

Note : Without using VPC we cannot create EC2 machines and RDS instances.

Note : To encourage beginners, 	AWS provided default VPC for us.



######################
VPC Terminology
######################

1) VPC
2) Types of IPs
3) CIDR blocks
4) subnets
5) Route Tables
6) Internet gateway
7) NAT Gateway
8) Security Group
9) NACL
10) VPC Peering




#######################
Types of IPs
#####################

=> For every electronic device one IP address is required to connect with internet.

=> IP addresss are divided into 2 types

    1) IPv4

    2) IPv6

NOte : Earlier we have only IPV4

-> IP ranges we will do it with CIDR

=> CIDR stands for Class for Inter Domain Change.



##################
IPV4
##################

=> IPV4 addresses are 32-bit numeric values divided into 4 octets

=> every octet is divided using period (.)

=> Ex : 

=> It is the most widely used IP version and supports approximately 4.3 billion unique addresses.

=> However due to the increasing number of devices connected to the internet, IPV4 addresses are running out, leading to the adoption of IPV6



##################
IPV6
##################


=> IPv6 addresses are 128-bit alphanumeric addresses written in 8 sets of 4 hexadecimal digits separated by colons


  Ex :

=> IPv6 provides a significantly larger addresses space than IPv4
 
=-> It was introduced to overcome the IPv4 addresses exhaustion issue and support the growing number of internet connected devices.


#################
VPC sizing
#################

=> The process of deciding no.of IPs required for our VPC.

=> Sizing will be calculated in 2 power

Ex:
 
   10.0.0.0/16 => 2 power (32-16) => 2 power (16) => 65 536

   10.0.0.0/32 => 2 power (32-32) => 2 power (0) => 1

   10.0.0.0/31 => 2 power (32-31) => 2 power (1) => 2

   10.0.0.0/30 => 2 power (32-30) => 2 power (2) => 4

   10.0.0.0/24 => 2 power (32-24) => 2 power (8)= > 256

   10.0.0.0/29 => 2 power (32-29) => 2 power (3) => 8

   10.0.0.0/28 => 2 power (32-28) => 2 power (4) => 16


Note : In general for VPC we will use "/16" and for subnets we will use "/24"

===========================================================
Assignment : Do some research on IPv4 classes
===========================================================



===========================================================================================================================================================================================

                                                                        29-Aug-24 IAM
                                                                   ========================





NO NOTES FOR TODAY.. ONLY CONCEPT EXPLAINATION


===========================================================================================================================================================================================

                                                                        30-Aug-24 IAM
                                                                   ========================



VPC : It provides network required for our resources.

CIDR : Its used to decide IP ranges

Subnets : Its a partition in VPC

    1) Public subnet

    2) Private subnet


Note-1 : The subnet which contains internet is called public subnet. Incoming and out going traffic is allowed.

 
Note-2 :  The subnet which does not have  internet is private subnet. No incoming and no outgoing.



Route Table  : Its used to configure routing rules for the tables.

Internet gateway : It provides incoming and outgoing network access for our VPC

Note -1 : If we add IGT to out=r route table, then that subnet becomes public subnet.

NAT Gateway : It provides only outgoing access for subnet resources.

NOte: NAT Gateway is commercial. Bill will be generated.

Security Group : To configure inbound and outbound rules. It works at resources level.

Note : SG supports only allow rules.

NACL : Network Access Control List. Its used to configure  both ALLOW and DENY rules at subnet level. NACL rules are applicable for all the resources available in subnet.

VPC peering : Establishing connection between 2 VPC's.



=============================================
How many VPC can we create in one region
==============================================

=> In AWS we can create upto 5 Vertual Private Clouds (VPCs) per region by default.

If we want more VPC's then we need to request AWS support.


##########################
VPC Labtask for today
##########################

Step-1) Create VPC
 
    (It will create one route table by default. Name it as "ashokit-private-rt")


Use VPC CIDR as : 10.0.0.0/16

Step-2) Create internet gate way and attach it to our VPC.

Step-3) Create 2 subnets (public and private subnets)

   public subnet CIDR : 10.0.0.0/24

   private subnet CIDR : 10.0.1.0/24


Step-4) Create one new Route table (Name it as public route table)

Step-5) Perform subnet associations with route tables


         public-rt =========================> public-sn

         private-rt ========================> private-sn


Step-6) Attach IGT to public RT to make that subnet as public.


Step-7) Craete one EC-2 VM in public subnet and create another in private subnet.

Step-8) Use SSH client and try to connect with both EC-2 VMs


Note : We should be able to connect to EC-2 VM created in public subnet and we should not be able to connect with EC-2 VM created in private Subnet.

=============================================================================
Step-9) Connect with private EC-2 from public EC2 using SSH connection
=============================================================================

Note : As both EC2 instances are available under same VPC, we should be able to access one machine to another.


============================================================================================================================================================================================

                                                                        31-Aug-24 IAM
                                                                   ========================
---
Procedure to Access :
---

=> Upload pem file into public EC2 machine (in mobaxterm we have upload option)

=> Execute below command to provide permission to access pem file.

$ chmod 400 <pem-file-name>

=> Execute below command to make SSH connection from public EC2 to private EC2

$ ssh -i "pem-file-name" ec2-user@private-ec2-vm-private-ip

   Ex : ssh -i "ashokitnewket.pem" ec2-user@65.2.73.111

Note : It should establish connection (This is internet connection)

=> try to ping google from private EC2 (It should allow because IGT is not available).

Note : For ex we want to download software or code from internet to our private subnet EC2-VM then we should allow only outgoing traffic access.

NOte: To achieve above requirement, we can use NAT gateway concept.


================================
VPC with NAT gateway lab task
================================

1) Create NAT gateway in public subnet

2) Add Nat gateway in "private-subet-route-table"

3) After NAt gateway attachment we should be able to ping google from private subnet ec2 VM also.

Note : Delete Elastic IP and NAT gateway after practice.


===========================================================================================================================================================================================

                                                                        02-Sep-24 IAM
                                                                   ========================
==============================
What is VPC peering
=============================

=> In general VPC will provide isolated network for the resources in AWS cloud.

=> 	If we create resources in private subnet then we cont access them outside.

=> If we have one requirement to access one VPC resources in another VPC then we can use VPC peering concept.

              peering
    VPC-1 =====================VPC-2


Note : VPC peering we can establish between same account VPC's and different account VPCs also.

=> When we establishing VPC peering both VPOC CIDR range should be different.

   Ashok AWS account (Default VPC) ---------------------------> Raju AWS account (Default VPC)

   My default VPC CIDR : 172.31.0.0./16

   My custom VPC CIDR : 10.0.0.0/16


Note: We can establish VPC peering for above 2 VPCs because their IP ranges are different hence no IP Collisions.


==========================================
Procedure to Establish VPC peering
==========================================

=> On the lest navigations panel under VPC -> peering connections.

=> Create VPC peering request

VPC peering (requester) = ashoikit_aws_custom_vpc

VPC peering (accepter)  = default_vpc

=> Now you would see the status Pending Acceptance which means, requester has sent a request to a peer now targer VPC needs to accept the request.

=> Go to VPC peering -> Click on Actions -> Accept Request

=> we need to make entries in the Route Tables

=> Now navigate to Route Tables, in default VPC RT -> Edit routes

Note : By default we will have local + igw, now we need to add custom VPC CIDR

############## Default VPC Route Table should have below 3 routes ##################

Local  : 172.31.0.0/16

IGW : 0.0.0.0/0

VPC Peering : 10.0.0.0/16

=> Now navigate to route tables -> Custom VPC RT -> Edit rules.

Note : By default we will have local + igw, now we need to add custom VPC CIDR

############## Custom VPC Route Table should have below 3 routes 	##############

Local : 10.0.0.0/16

IGW : 0.0.0.0/0

VPC peering : 172.31.0.0/16


###################  Allow Traffic in VPC Security groups ###########################


Edit security group of Default and custom VPC to allow traffic from each other.

Default VPC security group looks like

 SSH-22-aa

All traffic


Custom  VPC security group looks like

 SSH-22-all

All traffic


================================================Test VPC peering Connectivity=====================================================


# Ping ashokit-vpc EC2 VM private IP from default VPC VM

$ ping <private-ip>



# Ping ashokit-vpc EC2 VM private IP from default VPC VM

$ ping <private-ip>

===========================================================================================================================================================================================

                                                                        03-Sep-24 IAM
                                                                   ========================


1) EC2 : Virtual Servers

   - EBS Volumes and Snapshots
   - Load balancers
   - Keypairs
   - Security Groups
   - Auto Scaling Group

2) S3 : Simple Storage Service (Unlimited Storage, Object Based)

   - Buckets
   - storage
   - Versioning
   - Locking
   - Transfer Acceleration
   - Static Website Hosting
 
3) RDS : Relational Database Service

4) IAM : Identity and Access Management

   - Root Account
   - MFA
   - Users
   - Groups
   - Rules
   - Permissions
   - Control Access
   - Programmatic access
 
5) AWS CLI : Command Line Interface
 
6) Cloud Watch : For Monitoring Resources

7) SNS Service : Simple Notification Service

8) Elastic Beanstack : End-to-End Web app management (PaaS)

9) Lambdas : Serverless Computing (Pay as you use)

10) VPC : Virtual Private Cloud

11) Route 53


============
AWS EFS
============
 
=> AWS EFS lets you create scalable file storage to be used on EC2

=> EFS we can share with multiple EC2 instances.

=> If we have some configuration files of application and we want to access them in multiple EC2 instances then we can use EFS concept.


=========================
Step 1 :  to work with EFS
=========================

=> Login into AWS console

=> Go to services and select EFS under storage

=> Click on create file system.

=> file system id : fs-0339056b2db51a641

=> Create 2 EC2 instances (Amazon Linux AMI)

Note : 	Make EC2 VM security Group inbound rules having "NFS" protocol with 2049 port number.



============================
Step : 2 Mounting EFS on EC2
===========================

# Login to EC2 instance and install the NFS client
$ sudo yum install -y amazon-efs-utils


# Lets create a folder where you want to mount the files
$ sudo mkdir efsdir


Note : Make EC2 VM security group inbound rules having "NFS" protocol with 2049 port number.

# Mount EFS file system (Make sure you changed file system ID)
$ sudo mount -t efs -o tls fs-0339056b2db51a641:/ efsdir

# change the directory to the mount point that is created above using the command
$ cd efsdir

# Create a sample test file
$ sudo touch myfile.txt

# Run Is command to list the content of directory.

================================================================
Note : Create another EC2 instance and mount EFS file system
================================================================

# Login to EC2 instance and install the NFS client
$ sudo yum install -y amazon-efs-utils


# Lets create a folder where you want to mount the EFS
$ sudo mkdir efsdir

# MOunt EFS file system (Make sure you changes filesystem ID)
$ sudo mount -t efs -o tls fs-0339056b2db51a641:/ efsdir

# Change the directory to the mount point that is crated above using the command.
$ cd efsdir

# Create a sample text file
$ sudo touch myfile.txt

# Run Ls command to list the content of directory,


NOte : The files we have created in the first EC2 instance, should display in 2nd EC2 instance.

 
===========================================================================================================================================================================================

                                                                        04-Sep-24 IAM
                                                                   ========================
======================
Cloud Formation
======================

=> Cloud formation service is used to provision infrastructure in AWS cloud.

=> Cloud formation works based on 'Infrastructure as a Code' (IAAC).

=> Cloud formation supports JSON and YML configuration.

Note : If we craete infrastructure manually, it takes lot of time and its error prone.

=> If we design cloud formation template to create infrastructure tehn we can re-use the template multiple times.

Note : Cloud fromation service works in only AWS cloud.

Note : The alternate for 'Cloud Formation' service is 'TERRAFORM' tool.

=> Terraform works with almost all cloud platform available in the market.


=============================================
Creating EC2 instance using Cloud formation
=============================================

=> Go to AWS management Console and navigate to 'Cloud Formation'

=> Click the Create Stack and upload below template file.

---------------------------- Ec2 - creation - using - yml - file -------------------------

Description:  Ashok IT - Build Linux Web Server
Parameters:
  LatestAmiId:
    Description: AMI Linux EC2
    Type: 'AWS::SSM::Parameter::ValueAWS::EC2::Image::Id'
    Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2'
Resources:
  webserver1:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: "t2.micro"
      ImageId: !Ref LatestAmiId
      SecurityGroupIds:
        - !Ref WebserverSecurityGroup
      Tags:
        - Key: Name
          Value: webserver1
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash -xe
          yum update -y
          yum install httpd -y
          service httpd start
          chkconfig httpd on
          cd /var/www/html
          echo "<br>" >> index.html
          echo "<h2>Ashok IT EC2 Linux Demo</h2>" >>index.html
  WebserverSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable Port 80
      Tags:
      - Key: Name
        Value: webserver-sg
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 80
        CidrIp: 0.0.0.0/0

---

=> Verify EC2 dashboard, we can see EC2 instances created.

=> Access EC2 VM public in browser.


##################################
========================
Terraform
=========================
##################################

=> Developed by Hashicorp

=> To create provision infrastructure in cloud platform.

=> IAC software (Infrastructure as Code)

=> Supports almost all cloud platforms

=> Terraform will use HCL Language
 
      Hashicorp configuration Language

=> We can install Terraform in multiple operating systems.
 
     Ex : Windows. Linux..



==============================
Terraform VS Cloud formation
===============================
=> Cloud Formation is used to create infrastructure in only AWS cloud.

=> Terraform supports all cloud platforms available in the market.

===================================
Terraform installation in windows
===================================

Step -1 : Download terraform for windows & extract zip file.
 
           Note : we  can see terraform.exe file

Step-2 : Set path for terraform s/w in system environment variable.

Step : Download and install VS Code IDE to write terraform script.

setp-4 : verify the version in cmd
 
   ~terraform -v

 
===========================================================================================================================================================================================

                                                                        05-Sep-24 IAM
                                                                   ========================

=========================
Terraform Architecture
=========================

=> Terraform will use HCL Script to provision infrastructure in cloud platform.

=> We need to write HCL scripts and save it in .tf file

    .tf => init => fmt => validate => plan => apply => destroy


Terraform Commands
=====================

terraform init : initialize terraform script. (.tf file)

terraform fmt : Format terraform script indent spacing (optional )

Terraform validate : Verify terraform script syntax is valid or no.

terraform plan : Create execution plan for terraform script.

terraform apply : Create actual resources in cloud based on plan.

   Note : tfstate file will be created to track the resources created with our script.

terraform destroy : Its used to delete the resources created with our script.


Terraform AWS documentation : https://registry.terraform.io/providers/hashicorp/azure/latest/docs
=========================================
Terraform Script to create EC2 instance
=========================================


provider "aws" {

region = "us-east-2"
A_key = 
sKey = "

}

resource  "aws_instance" "ashokit_linux_vm" {
count = "1"
ami ="ami-06971c49acd687c30"
instance_type="t2.micro"
key_name ="awslab"
security_group=["default"]
tags {

 Name ="LinuxVM"
}
}


---


$ terraform init

$ terraform validate

$ terraform fmt

$ terraform plan

$ terraform apply --auto-approve

$ terraform destroy --auto-approve



===========================================================================================================================================================================================

                                                                        06-Sep-24 IAM
                                                                   ========================
Variables in Terraform
============================

=> Variables are used to store the data in key-value pair.

   id = 101
   name = ashokit

=> We can remove hardcoded values from resources script using variables.

=> Variables we  can maintain in separate .tf file


  Ex : var.tf

  variable "ami" {
 description = "amazon machine image id"
 default = }

variable "instance_type" {
 description = "reprasents ec2 instance type"
 default = "t2.micro"

}

=> we can access variables in our resources script like below.

resource "aws_instance" "ashokit_ec2_vm"{

    ami = "${var.ami}"
    instance_type = "${var.instance_type}"
    key_name = "${var.key_name}"
    security_group = ["default]

    tags ={
        Name ="AIT-Linux-VM"
    }
}

================================================================


types of variables in terraform
================================

1) Input variables

2) Output variables

=> Input variables are used to supply values to the terraform script

  Ex : 	ami, instance_type, keyname , security_group


=> Output variables are used to get the values from the terraform script after execution.

Ex-1 :  After EC2 VM created, print EC2 VM public IP

Ex-2 : After S3 bucket got created print bucket info.

Ex-3 : After RDS instance got created, print DB endpoint

Ex-4 : After IAM user got crated, print IAM user info.

 
Input variables
======================


variable "ami" {
  description = "amazon machine image id"
  default     = }

variable "instance_type" {
  description = "reprasents ec2 instance type"
  default     = "t2.medium"

}

variable "key_name" {
  description = "key pain name"
  default     = "awslab"
}



Output variables
==================

output "EC2_VM_Public_Ip" {
  value = aws_instance.ashokit_ec2_vm.public_ip
}

output "EC2_VM_Private_IP" {
  value = aws_instance.ashokit_ec2_vm.private_ip
}

output "EC2_VM_Subnet_Id" {
    value =aws_instance.ashokit_ec2_vm.subnet_id
}


Create multiple ec2 instances with different tag names.
=======================================================

locals {
 instance_count =3
instance_tags ={
 Name ="Dev-Server"
},

{
 Name ="QA-Server"
},
 Name ="UAT-Server"
{

}

======================================resource "aws_instance" "ashokit_ec2_vm" {

  ami             = var.ami
  instance_type   = var.instance_type
  key_name        = var.key_name
  security_groups = ["default"]

  tags = locals.instance_tags[count.index]
-

---
Output "instance_ids"{

value = aws_instance.ashokit_Ec2_Vm [*].public_ip
}

Assignment
==================================================

1) Create S3 bucket

2) Create RDS Instance

3) Create IAM User

4) Create Custom VPC

5w) Create EC2 instance using custom VPC




===========================================================================================================================================================================================

                                                                        09-Sep-24 IAM
                                                                   ========================

 Terrafom Modules
======================

=> A Terraform module is a set of terraform configuration files available in a single directory.

=> one module can contain one or more .tf files.

  01-peoject
    -main.tf
    -input.tf
    -output.tf

=> One module can have any number of child modules in terraform.

Ex : Inside project we can take  ec2, s3, RDS, as child modules.

             shokit
                 -ec2
                    -main.tf
                    -input.tf
                    -output.tf
                 -RDS
                    -main.tf
                    -input.tf
                    -output.tf
                 -main.tf
                 -provider.tf
                 -output.tf

NOte : Using tearrform module we can achieve reusability

Note : We will run terraform commands from root module and root module will invoke child module for execution.


Terraform project setip using Module
======================================

Step-1 : Create project directory

  Ex : 03-App

Step-2 : Create "Module" directory inside projec directory

  Ex : 03-App
           -Module

Step-3 : Create EC2 and S3 directories inside module.

  Ex : -Module
           -EC2
           -S3

Step-4 : Create terraform scripts inside EC2 directory.
 
         input.tf
         main.tf
         output.tf

Step-5 : Create terraform scripts inside S3 directory.
 
         input.tf
         main.tf
         output.tf

Step-6 : Create provider.tf file in root module
 
Step-7 : create main.tf file in root module and invoke child module from root module.
 
Step-8 : Create output.tf in project root module and access child module related outputs

 
==============================================

https://github.com/ashokitschool/Terraform_Projects.git



===========================================================================================================================================================================================

                                                                        11-Sep-24 IAM
                                                                   ========================


=================================
Environments of the projects
================================
=> Environment means the platform that is required o run out application

   Ex: Servers, database , storage, network ,

=> One project contains multiple environments

   Ex: DEV, SIT or QA, UAT, PILOT, POD

DEV : Developers will use it for integrated testing.

SIT/ QA : Testers will use it for code integrated testing

UAT : Client will use it for accepting testing.

PILOT : Pre-Production testing

POD : Live environment

Note : In real-time from environment to environment infrastructure resources configuration must be different.

  Ex : For Non Prod : t2.medium instances are required

       for Prod : t2.large instance are required.

=================================================================

input-dev.tf : input variables for dev environment

input-sit.tf : input variables for sit environment

input-uat.tf : input variables for uat environment

input-pilot.tf : input variables for pilot environment

input-prod.tf : input variables for prod environment

==========================================================================

=> When we are executing terraform apply command we can pass input variables file like below.

# Create infrastructure for DEV environment
$ terraform apply --var-file=input-dev.tf

# Create infrastructure for QA environment
$ terraform apply --var-file=input-qa.tf


Note : With this we can achieve loosely coupling and we can achieve script re-usability



=================================
Workspace in terraform
==============================

=> To manage infrastructure for multiple environments we will use terraform workspace concept.

=> When we use work space it will maintain separate state file for every environment/ work place.

Note : We can execute same script for multiple environments.

$ terraform workspace show

$ terraform workspace new dev

$ terraform workspace new qa

$ terraform workspace new uat

$ terraform workspace new prod

$ terraform workspace list

$ terraform workspace select dev


===========================================================================================================================================================================================

                                                                        13-Sep-24 IAM
                                                                ===================================


==========================================
Working with terraform workspace concept
===========================================
 
Step-1 :Create terraform project

Step-2 : Create provide.tf file and configure provider details

Step-3 : Create input variables files based on environments and configure variable values

     Ex : dev.tfvars
          qa.tfvars
          prod.tfvars

Step-4 : Craete main resources script file

Step-5 : Create output variables file

Step-6 : Create workspaces and select workspace

Step-7: Run script and check state files
 
      $ terraform apply --var-file=dev.tfvars --auto-approve

Note : When we use workspace concept, it will maintain separate state file for every environment.


===================================================
What is taint and untaint in terraform
==================================================

=> Terraform taint is used to replace the resource when we apply the resource next time.

=> For example we have created two resources like below.

           resource "aws_instance" "vm1" {
          // configuration
          }

     resource “aws_s3_bucket” “abt1” {
     // configuration
       }

=> After sometime we realized that EC2 VM got damaged then we can taint that EC2 Vm using below command.

$ terraform taint aws_instance.vm1

$ terraform apply --auto-approve

Note : The alternate for “taint” is “replace”


teffaform apply --auto-approve

$tarraform apply -replace=”aws_instance.vm1” --auto-approve

=========================
Terraform Summery
========================

1) Infrastructure as code (IAC)

2) Terraform introduction

3) Terraform setup

4) Terraform architecture

5) Terraform scripts using HCL

6) Variable (input & Output)

7) Terraform module

8) Project environment

9) Terraform workspaces

10) Resource taint or replace

11) Terraform Vault



===========================================================================================================================================================================================

                                                                        17-Sep-24 IAM
                                                                ===================================


=========================
Configuration management
=========================

=> Installing required software in the machine

=> Copy required files from one machine to another machine.

=> OS patching/ updates

=> We can perform configuration management in 2 ways

   1) Manual Configuration management

   2) Automated Configuration Management


Problems with manual configuration management
================================================

1) Time taking process

2) Repeated work

3) Human mistakes

=> To overcome these problems we are going to automate configuration management in the project.

=> To automate configuration management we have several tools

   1) puppet
   2) Chef
   3) Ansible (Trending)


===============================
What is Ansible
===========================

=> Its an open source software developed by Michel DeHaan and its ownership is under RedHat

=> Ansible is written in python language

=> Ansible is an automated tool that provides a way to define configuration as code


=========================
What Ansible can do ?
=========================

1) Configuration Management

2) App Deployement


======================
Ansible Architecture
======================

1) Control Nodes

2) managed nodes

3) Host Inventory file

4) Playbooks

=> The machine which contains ansible software is called as controlling node.

=> The machines which are managed my controlling node are called as Manages Nodes.

=> Host inventory file contains manages nodes information.

=> Playbook is YML/YAML which contains set of tasks.

URL : https://github.com/ashokitschool/DevOps-Documents/blob/main/11-Ansible-Setup.md


ssh-copy-id ansible@172.31.15.244


ssh-copy-id ansible@172.31.4.103


[webservers]
172.31.15.244

[dbservers]
172.31.4.103



======================
Ansible Setup
========================

# Ansible Setup in Amazon Linux VMs #

## Step-0: Create 3 Amazon Linux VMs in AWS (Free Tier Eligible - t2.micro)

1 - Control Node <br/>
2 - Managed Nodes / host Nodes

Note: Connect to all 3 VMs using MobaXterm

## Step-1: Setup User and Configure user in sudoers file and update SSHD Config File. Execute below commands in all 3 VMs ##

### a) Create user ###
```



sudo useradd ansible



sudo passwd ansible



```
### b) Configure user in sudoers file ###

sudo visudo

```



ansible ALL=(ALL) NOPASSWD: ALL



```

### c) Update sshd config file ###
```



sudo vi /etc/ssh/sshd\\\_config



```
-> comment PasswordAuthentication no
-> PermitEmptyPasswords yes

### d) Restart the server ###
```   



sudo service sshd restart



```
Note: Do the above steps in all the 3 machines

## Step-2: Install Ansible in Control Node ##

### a) Switch to Ansible user ###
```



sudo su ansible



cd ~



```
### b) Install Python ###
```



\&nbsp; sudo yum install python3 -y



```
### c) Check python version  ###
```



python3 --version



```
### d) Install PIP (It is a python package manager) ###
```



sudo yum -y install python3-pip



```
### e) Install Ansible using Python PIP ###
```



pip3 install ansible --user



```
### f) Verify ansible version  ###
```



ansible --version



```
### g)  Create ansible folder under /etc ###
```



sudo mkdir /etc/ansible 



```

## Step-3: Generate SSH Key In Control Node and  Copy SSH key into Managed Nodes ##

### a) Switch to ansible user ###

```



sudo su ansible



```
### Generate ssh key using below command ###
```



ssh-keygen



```
### b) Copy it to Managed Nodes as ansible user ###
```



ssh-copy-id ansible@<ManagedNode-Private-IP>



```
Ex : $ ssh-copy-id  
Note: Repeat above command by updating HOST IP for all the managed Servers.

## Step-4: Update Host Inventory in Ansible Server to add managed node servers details ##
```



sudo vi /etc/ansible/hosts



```
[webservers] <br/>
172.31.47.247
<br/>
[dbservers] <br/>
172.31.44.90

## Step-5: Test Connectivity ##
```



ansible all -m ping



```

========================================================================================================================================================================================


                                                                        18-Sep-24 IAM
                                                                ===================================

Ansible Ad-Hoc Commands
==========================

=> To run Ad-Hoc commands we use below syntaxes.

 $ ansible [All/ Group-name/ private IP] -m <module> -a <args>

Ex :  $ ansible all -m ping

      $ ansible webservers -m ping

      $ ansible dbservers -m ping

=> We have several servers modules in ansible to perform configuration management.

   1) ping -> To check connectivity

   2) shell -> to execute a command

   3) yum / apt -> To install some software

   4) service -> start or restart a service

   5) Copy -> To copy files from one mch to another machine

 
   $ ansible all -m shell -a date

   $ ansible all -m yum -a “name=git”

   $ ansible webserver -m yum -a “name-httpd”


Assignment
==================
=> Ansible module and purpose

=> What is YML

========================================================================================================================================================================================


                                                                        18-Sep-24 IAM
                                                                ===================================






========================================================================================================================================================================================


                                                                        19-Sep-24 IAM
                                                                ===================================

Ansible Playbook
====================

=> Playbook is a YAML file

=> Playbook contains one or more tasks

=> Using playbook we can define what tasks to be performed and where to be performed

=> We will give playbook as input for the ansible control node to perform tasks in managed nodes

Note : To write ansible playbook we should learn YAML first.


=======================
YML or YAML
===================

=> YML stands for Yet Another Language

=> Its used to store data in human and machine readable format.

=> YML or YAML files will have extension as .yml

=> Official website ; https://yaml.org/


=======================
01: Sample yaml file data
=======================

Note : indent spacing is very important.

id: 101
name: Ashok
gender: male
hobbies:
    -music
    -chess
    -cricket

=======================
02: Sample yaml file data
=======================

person:
 id: 101
 name: Ashok
 address:
   city: hyd
   state: Telangana
   country: India
hobbies:
   -chess
   -cricket
   -tv
=======================

Task
=======
 
Write yml file to represent employee data with company and job details

emp -> id, name , company and job

company name

job -> exp, type (permanent, contract)


yaml
=====
---
employee:
  id: 101
  name: Vamshi
  company:
   companyname: coforge
  job:
   experience: 6years
   type: permanent
...
===================================


Website to validate YML syntax : https://www.yamllint.com

===============================================================================================================================

Writing playbooks
======================

=> Playbook contains 3 sections

1) Host section

2) variable section

3) task section.

=> Host section represents target machines to execute tasks.

=> Variables section is sued to declare variables required for playbook execution.

=> task section is used to define what operations we want to perform using ansible.

Note : In single playbook we can perform multiple tasks also.

=> To execute playbook we can use below  syntax.

  $ ansible-playbook <playbook-yml-file>


==================================
playbook to ping managed nodes
=================================

---
- hosts: all
  tasks:
  - name: ping all managed nodes
    ping:
...

# it will check the syntax of the playbook
$ ansible-playbook <playbook-file> --syntax-check

# It will display which hosts will be effected by the playbook before run
$ ansible-playbook <playbook-file> --list-hosts

# Run the playbook using below command
$ ansible-playbook <playbook-file>

# It executes one-step-at-a-time, confirm each task before running with YES, NO, Continue.
$ansible playbook <playbook file > --step

# Run the playbook in verbose mode
$ $ ansible-playbook <playbook-file> --vvv



---
- hosts: all
  tasks:
    - name: create a file
      file:
        path: /home/ansible/ashokit.txt
        state: touch
...

---
gitHub.com/ashokitschool

search for ansible playbook sample

=============================================
---
id: 101
name: ashok
gender: Male
hobbies:
 -Music
 -Cricket
 -Chess
address:
 city: hyd
 state: TG
 country: India
...
==============================================

---
- hosts: all
  tasks:
    - name: create a file
      file:
        path: /home/ansible/ashokit.txt
        state: touch
...
====================================
---
- hosts:
  tasks:
  - name: copy content to file
    copy: content="hello world" dest="/home/ansible/ashokit.txt"
...

==========================================================


---
- hosts: all
  become: true
  tasks:
    - name: install httpd
      yum:
        name: httpd
        state: latest

    - name: copy index.html file
      copy:
        src: index.html
        dest: /var/www/html/index.html

    - name: start and enable httpd
      service:
        name: httpd
        state: started
        enabled: true

==================================================================================



========================================================================================================================================================================================


                                                                        -Sep-23

                                                            ==================================
Handlers and tags
====================

=> In playbook all tasks will be executed by default nonsequential order.

=> using handler we can execute tasks based on other tasks status.

Note : If 2nd task status is changed then only execute 3rd task.

=> handlers are used to notify 	the tasks to execute

‘notify’ keyword we will use to inform handler to execute

=> Using tag we can map task to a tag name.

=> Using tag name we can execute particular task and we can skip particular task also.

===================================

Ex :

# To display all tags available in the playbook
$ ansible-playbook handles_tags.yml --list-tags

# Execute a task whose tag name is install
$ ansible-playbook handles_tags.yml --tags “install”

# Execute the tags whose tag names are install and copy
$ ansibl-playbook handles-tags --tags “install, copy”

# Execute all the tasks in playbook by skipping installing task
$ ansible-playbook handles_tags.yml --skip-tags “install, copy”

=====================================================================

---
- hosts: all
  become: true
  tasks:
    - name: install httpd package
      yum:
        name: httpd
        state: latest
      tags:
        - install

    - name: copy index.html file
      copy:
        src: index.html
        dest: /var/www/html/index.html
      tags:
        - copy
      notify:
        - "start httpd server"

  handlers:
    - name: start httpd server
      service:
        name: httpd
        state: started
        enabled: true
      tags:
        - start
...
==========================================================


====================
Variables in Ansible
======================

=> variables are used to store the data
 
   Ex: di = 101
       name= ashok
       age=20
       gender=male

=> In ansible we can use variables in 4 ways

   1) Runtime variables

   2) Playbook variable

   3) Group variable

   4) Host variable


Runtime variable
=====================

=> We can pass variable value in runtime like below

---
 - hosts:
   become: true
   tasks:
   - name: install package
     yum:
      name: “{{package_name}}”
      state: latest
...

$ansible-playbook <yml>  --extra-vars package_name=httpd


===================
Playbook  variables
=======================

---
 - hosts:
   become: true
   vars:
     package_name: httpd
   tasks:
   - name: install package
     yum:
      name: “{{package_name}}”
      state: latest
...


===========================================================

Requirement
===================

write ansible playbook of install below software

webserver group : install java

dbserver group : install MySQL


---
 - hosts: all
   become: true
     tasks:
   - name: install package
     yum:
      name: “{{package_name}}”
      state: latest
...

=> to achieve above requirement we need to use group var concept.

============================================================


========================================================================================================================================================================================


                                                                        24-Sep-24
                                                                   =======================

===================
Group Vars
===================

=> Group vars concept is used to specify variable value for group of managed nodes as per inventory file.

=> Managed nodes we are configuring host inventory file like below.

[webservers]
webserver1 ansible_host=172.31.0.95
webserver2 ansible_host=172.31.0.96

[dbservers]
172.31.23.45
172.31.23.55


=> While executing above playbook for webservers group i want to pass one package name and for dbservers group i want to pass another package name.

Note : We need to create variables based on group name.

Ex: webservers.yml
 
    dbservers.yml

Note : group_vars related yml files we should  create in host inventory file location.

Host inventory file location : /etc/ansible/hosts

webservers variable file : /etc/ansible/group_vars/webservers.yml

dbservers variable file : /etc/ansible/group_vars/dbservers.yml



====================
Host Variable
====================

=> Host variables are used to specify variable value at host level(or) machine level

=> host vars we will create in below location

location : /etc/ansible/host_vars

 webserver1.yml

    package_name: java

dbserver2.yml

   package_name: python


Note 1: host variables will take precedence over group variables.

Note 2 : Variable defined in playbook override both host-vars and group_vars


==============================
Ansible Vault
=====================

=> Its used to secure out playbooks

=> Using ansible vault concept, we can encrypt and decrypt out playbooks

   Encryption : Convert data from readable format to un-readable format

   decryption : convert data from unreadable format to readable format


# Encrypt out playbook
$ ansible-vault encrypt <file.yml>

Note : to  encrypt a playbook we need to set one vault password

# see encrypted playbook
$ cat <file.yml>

# see original content of playbook
$ ansible-vault view <file.yml>

# To edit encrypted playbook
$ ansible-vault edit <file.yml>

# To run encrypted playbook
$ ansible-playbook <file.yml> --ask-vault-pass

# decrypt playbook
$ ansible-vault decrypt <file.yml>

 

========================================================================================================================================================================================


                                                                        25-Sep-24
                                                                   =======================

24-set-24 : same video uploaded again



========================================================================================================================================================================================


                                                                        26-Sep-24
                                                                   =======================


================
Ansible Roles
================

=> If we add more functionalities in the playbook then it will become very lengthy and it will be difficult to manage and maintain that playbook.

=> Using roles concept we can break down large play books into smaller chunks

=> Roles will provide abstraction for ansible configuration in a modular and reusable format.


Ex :

---
- hosts: all
  become: true
  tasks:
    - name: install httpd package
      yum:
        name: httpd
        state: latest
      tags:
        - install

    - name: copy index.html file
      copy:
        src: index.html
        dest: /var/www/html/index.html
      tags:
        - copy
      notify:
        - "start httpd server"

  handlers:
    - name: start httpd server
      service:
        name: httpd
        state: started
        enabled: true
      tags:
        - start
...
=============================

to install tree

$ sudo yum install tree


=============================================================================

Working with Ansible roles
==============================


=> To create a role we can use below command

syntax: ansible-galaxy init <role-name>

Step 1: Connect with control node and switch to ansible user

$ sudo su ansible
$ cd ~

step 2: Create a role using ansible galaxy

$mkdir roles
$ cd roles
$ ansible-galaxy init apache
$ sudo yum install tree
$ tree apache

step 3: Create 	tasks inside tasks/main.yml


---
#tasks files for apache
- name: install httpd
  yum:
   name: httpd
   state: latest
-name: copy index.html
 copy:
   src=index.hyml
   dest=/var/www/html
 notify:
   -restart httpd
...



step 4 : copy required file into files directory

Note: Keep index.html files in files directory


step 5: configure handlers in handlers/main.yml

# handlers file for apache

- name: restart apache
  service:
   name: httpd
   state: restarted

step 6: Create main playbook to invoke 	roles using role name

  $ cd/home/ansible


---
- hosts: all
  become: true
  roles:
   - apache
...


========================================================================================================================================================================================

                                                                        27-Sep-24
                                                                   =======================
===================
Gather facts
=================

=> In ansible gathering facts refers to the process of collecting information about the target hosts before executing tasks.

   Ex: OS, memory, cpu architecture   etc..

=> This information is collected automatically using "setup" module.

---
- hosts: all
  gather_facts: no
  tasks:
  - name: ping all managed nodes
    ping:
...

---
- hosts: localhost
  gather_facts: yes
  tasks:
  - name: print the name of OS family
    debug:
      msg: "The OS is {{ansible_os_family}}"
  - name: print memory info
    debug:
      msg: "total memory is {{ansible_facts['memtotal_mb']}} MB."
...



================================
What is debug keyword  in ansible
================================

=> Its used to print a message when playbook is getting executed

---
- hosts: localhost
  gather_facts: yes
  tasks:
    - name: print the name of OS family
      debug:
        msg: "The OS is {{ ansible_facts['os_family'] }}"
 
    - name: print memory info
      debug:
        msg: "Total memory is {{ ansible_facts['memtotal_mb'] }} MB."
...




====================================
What is register module in Ansible
====================================
=> register keyword in ansible allow you to capture the output of a task and store it into a variable for later use.

Note: One task output can be registered and we can use it in another task like below


---
hosts: localhost
tasks:
- name: Run a command to get current date
  command: date
  register: date_output

...


===========================================


---
hosts: localhost
tasks:
- name: Run a command to get current date
  command: date
  register: date_output

- name: print date command output
  debug:
  msg: "The current date is => {{ date_output.stdout}}"
...


===================================================

---
hosts: localhost
tasks:
- name: Run a command to get current date
  command: date
  register: date_output

...


===========================================================


---
hosts: localhost
tasks:
- name: Run a command to get current date
  command: date
  register: date_output

- name: print date command output
  debug:
  msg: "The current date is => {{ date_output.stdout}}"
...

=======================================================================

---
hosts: localhost
tasks:
- name: Run a command to get current date
  command: date
  register: date_output

- name: print date command output
  debug:
  msg: "The current date is => {{ date_output.stdout}}"

- name: check command is success or not
  debug:
    msg: "command execution successful"
  when: date_output.rc==0

 
...




====================================================================
Write a playbook to install Java in different OS family machines
====================================================================

Write  a playbook to install java in all managed nodes

MN-1: amazon Linux => RedHat family

MN-2: ubuntu => Debian Family


---
- hosts: all
  tasks:
  - name: install java in RedHat family
    yum:
     name: java
     state: latest
    when: ansible_os_family='Red Hat'

- name: install java in Debian Family
  apt:
   name: java
    state: latest
   when: ansible_os_family =='Debian'


===================================================================
Error Handling in Ansible
==============================

=> If we get any error in task execution then playbook execution will be terminated abnormally (in middle)

=> To achieve graceful termination we need to handle that error

Error Code
=================
---
- hosts: all
  tasks:
  - name: run a Linux command
    command: dates

 - name: this is second task
   debug:
   msg: "second task executed.."

--

Ignore error code
=====================

---
- hosts: all
  tasks:
  - name: run a Linux command
    command: dates
    ignore_errors: yes

 - name: this is second task
   debug:
   msg: "second task executed.."

...

=========================================

---
- hosts: all
  tasks:
  - name: this is first task
    command: dates
    register: dates_output
    ignore_errors: yes

 - name: this is second task
   debug:
   msg: "second task executed.."
   when: dates_output.rc==0

 - name: this is third task
   debug:
     var: dates_output

...


=================================================================================================================================
Requirement : Write a playbook if maven is available and if it is not available then install maven software in control node

====================================================================================================================================

---
- hosts: localhost
  become: true
  tasks:
  - name: check maven version
    command: maven --version
    register: output

  - name: print output of the first task
    debug:
      var: output

  - name: install maven
    yum:
     name: maven
     state: latest
   when: output.failed
...
=======================================

---
- hosts: localhost
  become: true
  tasks:
  - name: check maven version
    command: maven --version
    register: output
    ignore_errors: yes

  - name: print output of the first task
    debug:
      var: output

  - name: install maven
    yum:
     name: maven
     state: latest
   when: output.failed
...



Recap
===========================================================

1) What is configuration management

2) Ansible Introduction

3) Ansible Architecture

4) Ansible Setup

5) Ansible Ad-Hoc commands

6) Ansible Modules

7) YML files

8) Playbooks

9) Handlers and tags

10) variables

11) Ansible Vault

12) Ansible roles


===============================================================
Assignment : How to create S3 bucket using ansible playbook
===============================================================




###########################################################################################################################################################################################
                                                                              07-Oct  |  DevOps
                                                                          ###################################

========================
Build Tool
=======================

=> Build tools are used to automate project build process.

=> Build process means convert project source code to executable format


==============================
Java Project Execution flow
================================

=> Developers will develop the source code for the project.

  Ex : .java files

=> We need to convert that source code into byte code using java compiler.

=> When we compile source code, it will converted to byte code.

     Ex : .class files

=> we need to package .class files as jar or war files.

  JAR : Java Archieve
 
  WAR : Web Archieve

=> Standalone apps will be packaged as JAR files

=> Web apps will be packaged as WAR file.


=======================
Java Build Tools
======================

1) ANT (Outdated)

2) Maven

3) Gradle


========================
Maven
========================

=> Its build tool

=> Its free and open source software developed by Apache organization.

=> Maven s/w is developed by using java language

=> Maven is used as java projects build automation tools.


=====================================
What we can do with maven ?
==================================

1) We can java project folder structure

2) We can download required libraries
 
    Ex : Hibernate, Spring boot, junit, log4j, kafka, redis, security, Jackson.....

3) We can compile source code of the project.
 
    .java file ---------------> .class file

4) We  Execute unit test cases of the project.

5) We can package our application as jar or war file

Note : The main aim of maven is to automate and simplify java projects build process.


=============================
Maven setup in Linux
=============================

# check maven software availability
$ mvn -version

# install maven
$ sudo yum install maven


==========================
Maven terminology
============================

1) Archetype : Reparents type of project we want to create

       quick-start : stand alone app (jar)
 
       web-app : web application (war)


2) groupId : represents company domain name

    Ex : in.ashokit
         com.ibm
         com.tcs

3) ArtifactId : represents project name

    Ex: sbi_net_banking_app

         ashokit_ecom
 

4) version
 
    ex: 1.0-snapshot (under development)
 
        1.0-release (delivered to production )


5) packaging : project executable format
 
     Ex : jar or war

6) dependencies

   ex: hibernate, Spring boot, junit, log4j


7) maven goals: To perform build process

   ex: compile
 
       test
 
       package
 
        install

        deploye


8) maven repository: Location where maven dependencies will be stored

          1) Central repository (public, managed by apache)

          2) remote repository  (private, company specific- nexus/ jfrog)

          3) Local repository (will be created in local system)



==================================================
Creating maven standalone application
==================================================

mvn archetype:generate -DgroupId=com.ashokit -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false



==================================================
Creating maven web application
==================================================

mvn archetype:generate -DgroupId=com.ashokit -DartifactId=my-web-app -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=false



=> Navigate into project directory and execute maven goals
 
   $mvn package

============================================================================================

maven Goals
====================

=> Maven Goals are used to perform project build process.

    Syntax : mvn <goal name>

Note : We need to execute mvn goals from project root directory (where pom.xml 	is available)
 
=> we have several maven goals like below

# Clean : To delete target directory

# compile: Compile source code of the project

       Note: It will convert .java files to .class files

       Note; It will generate target directory to store .class files.


# test : To execute project unit test code (junit)

       test =compile + test


# package: To package our project as jar / war file

       package =compile + test + package

Note : Application package will be stored into target directory

       Ex : mvn clean package

# install : To publish our project artifact (jar/war) into maven repository

   install = compile + test + package + install


=======================================
Maven Dependencies
=================================

=> Maven dependencies means libraries required for the project development

     Ex : Spring core, JUnit, hibernate...

Note : Developers will add required dependencies in project pom.xml file

=> We can find maven dependencies in www.mvnrepository.com


<dependency>
 <groupId>org.sorungframwork <groupId>
  <arctfactId>spring-core <arctfactId>
  <version>6.1.13 <version>
</dependency>


=> Add above dependency in project pom.xml (project object model) file under <dependencies > section and execute maven goals

    mvn clean package


============================
Maven Repositories
===========================

=> Its  a plase where maven dependencies will be stored

=> we have 3 types of repositories in maven

1) local repository

2) Central repository

3) Remote repository

=> Local repository will be created in our machine (.m2 folder)

=> Central repository will be maintained by our company to store shared libraries

=> Remote repository will be maintained by our company to store shared libraries.

  Note : To setup remote repositories we will use Nexus / JFrog softwares


===========================
Maven - summery
=============================

1) What is build tool and why
 
2) What is Java application build process

3) maven Introduction

4) Maven setup in windows

5) What we can do using maven

6) maven terminology

7) Maven project creation

8) Maven dependencies

9) Maven goals

10) Maven repositories  (.m2 is a local repository )


###########################################################################################################################################################################################
                                                                              09-Oct  |  DevOps
                                                                          ###################################

================
GitHub
==============

=> Its a version control software

=> Github is  a platform which is used to store project related files/code

=> In GitHub we can create source code repository to store project code

=> All the developers can connect to the project repository to store all the source code. (Code integration will become easy)

=> Code will be availabe at central location (easy access)

=> repository will moniter all code changes

    - WHo modified
 
    - When modifies

    - What modifies

    - Why modified


================================
Environment Setup
===============================

1) Create account in www.github.com

2) download and install git client software
 
    https://git-scm.com/downloads

3) Open git bash tool and configure your name and email using below commands

   $ git config --global user.name "vamshi"

   $ git config --global user.email "vamshie.dev@gmail.com"

Note : Configuring name and email is just one time process.

================================
What is Git hub repository
================================

=> Repository is a place where we can 	store project source code / files

=> For every project one repository will be created

=> We can creat 2 types of repositories in git hub

    1) Public repo (anybody can see & you can choose who can commit)

    2) private repo ( you choose who can see & who can commit)

project repo url : https://github.com/ellandulav/sbi_credit_card_app.git

=> project team members will connect with git repository using its url

=========================================
Difference between git and git hub
========================================

=> Its a client software which is used to communicate with hut hub repository

========================
Git Architecture
========================

1) Working tree

2) staging area

3) local repo

4) central repo

=> Working tree represents the place where we want to perform git operations.
 
    generally project directory will be considered as working tree.

Note: to represent/initialize working tree we will execute 'git init' command.

=> Staging area represents which files are eligible for commit. To add files to the

staging area we will use 'git add' command.

    git add <file name>

    git add *.txt

    git add *.java

    git add .

=> Local Repo represents the commits we have done using 'git commit' command

    git commit -m 'commit-msg'

Note : With 'git commit' only stages files will be committed to local repo.

=> Central repo will be available in git hub. All the team members changes will be integrated in central repo.

Note: To send local commits to central repo we will use git push command.

================
Git command
================

git config : to configure name & email

git init : To initialize working tree

git status : To check working tree status

git add ;To add files to staging area.

    git add f1.txt

git commit : To commit file from staging to local repo

   git commit -m "changes made"

git push : To send files from local repo to remote repo

   git push

git restore:

    1) To discard changes when 	the file is unstaged state.
 
              git add f1.txt

    2) To unstage the file when it is added to staging area.

              $ git restore --staged f1.txt

git log : to see commit history

git pull : To take latest changes from central repo to local repo

  git pull

git clone : to clone remote repo to local machine

      git clone <repo-url>

git rm : to remove files (rm + commit + push)

====================================================================================
echo "# sbi_mobile-banking-" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/ellandulav/sbi_mobile-banking-.git
git push -u origin main

=====================================================================================


###########################################################################################################################################################################################
                                                                              10-Oct  |  DevOps
                                                                          ###################################

Working with git GUI
=========================

command : git gui allows you to interact with git hub with user interface.

=> To remove a file permanently from central repo use below command

     git rm f1.txt

  git commit -m "message" --> git commit


============================
What is .gitignore file ?
=============================

=> .gitignore is used to exclude files & folders from our commit

Ex: in maven project we should not commit "target folder" to git repository hence we are giving this info to git using  .gitignore file.

Ex 2: In angular app we should not commit "node modules" folder



==============
Git branches
===============

=> In project multiple development teams work parallelly.

1) Bug fixing team

2) Enhancement team

3) change request team(CR)

4) Research and development team

5) prod support team.


=> When multiple teams work on a single repository, Then it will become very difficult 	to manage the code.

Note : To overcome this problem we will use "git branches" concept.

=> Branches are used to maintain multiple code base in a single repository

Note : We can create any no. of  branches in single repository.

=>If we have branches in git repo then multiple teams can work parallelly without effecting others teams code.


# Clone git repo (default main branch)
$ git clone https://github.com/ellandulav/sbicreditcards.git

# Clone git repo develop branch only
$ git clone -b develop https://github.com/ellandulav/sbicreditcards.git


# switch branch
$ git checkout <branch name>
$ git switch <branch name>

=============================
What is pull request
-=-==========================

=> Its uses to merge changes form one branch to another branch

                  PR
Ex ; develop ---------------> main
 
###########################################################################################################################################################################################
                                                                              15-Oct  |  DevOps
                                                                          ###################################



What is git stash
=====================

11:00 AM manager assigned a task AB-101

2:00 PM Completed half of the task (not fully completed )

3:00 Manager called and informed AB-103 is high priority

=> first complete AB-103 and then work in AB-101

NOte : In this situation we cannot delete 102 changes from working tree because 3 hours time spent and we cont commit also because changes are not staged.

Git stash : save working tree changes in temp area and make working tree clean.

git stash apply : Get stashed chages back to working tree

 
What is git fetch ?
===================

Git pull : It will download latest changes from central repo to local repo and merge with working tree also.

Note : with pull commad there is a chance of getting conflicts.


git fetch : It will donwload latest chaages from  central repo to local repo and it will not merge with working tree.

NOte : To merge changes from local repo to working tree we need to execute "git merge " command.


git pull = git fetch + git merge


What is git revert
==================

=> revert is used to remove changes from our central repo based on commit id

   git revert <commit id>

Note : when we execute revert command it will open editor for msg. Save and close it using "esc + :wq"

=> After revert command , we need to execute git push to publish revert operation to central repo.

What is git conflict
======================

=> when we execute git pull command, there is a chance of getting git conflicts

Note : If two team members working on same file and same line then it will get conflict.

=> When we merge one branch changes with another branch then there is a chance of getting git conflicts.

Note : 	When we get conflicts, the  its our responsibility to resolve those conflicts and commit to git hub without conflicts.

============================================================================================================
Bitbucket
===========

checkout ...................

==============================================================================================================

What is git fork
====================

=> It allows you to create a personal copy of someone else repository.

Note : In general we will use this forking concept for experimenting on open source projects.


=================================
How to remove git local commits
=================================

# Remove latest local repo commits and keep changes in working tree
$ git reset --soft HEAD~1

# Remove latest local repo commits and discard changes form working tree
$ git reset --hard HEAD~1

# remove last 3 local commits
git reset --soft HEAD~3


==========================
What is cherry-pick
=========================

=> In git, cherry pick is a command to apply the changes from one specific commit from one branch to another.

Ex : In develop branch we have 2 commits. If we create pull request to merge develop branch changes to main branch then it will merge all commits at once (full merge)

=> If we dont perform full merge we want to merge only particular commits from develop branch to main, then use 'get cherry-pick ' option.

  Ex : git cherry-pick <commit-id>


===================
What is git tag
===================

# show all tags
 git tag

# create a tag
 git tag -a <tag-name> -m <tag-message>

# push tags to central repo
 git push origin <tag-name>

# push all tags to central repo
 git push --tags

# delete tags
 git tag -d <tag-name>

flow
==========
git tag -a relase-1.0 -m "release 1.0"
git tag
git push origin release-1.0


# to check what is pushed before deadline
git checkout release-1.0


=> We can checkout particular tag using its name
 git checkout <tag-name>

Note : To come out from tag state we can use below command

git switch -

==========================
git merge VS git rebase
==========================

=> these commands are used to merge one branch commits to another branch

 Ex :

=> We have committed changes to develop branch now we want to merge develop branch changes to main branch

 git switch main

 git merge develop (or) get rebase develop

merge : Creates new commits and combines and preserve commit history.

rebase : Reapplies commits on top of our branch. It will not preserve commits history.


==================================
Real-time work flow
================================

=>  As a devOps engineer we are responsible to merge source code repositories in the projects

=> Development team will create a JIRA story for it repo creation and will assign to devOps teams with development manager approval

=> Once request got approved we should create git repo and we need to share git repo url with development team.

=> Development team will integrate code in git repo and we need to create multiple branches.

Note : DevOps team will decide branching strategy. that means development code should be there in which branch. and which branch code will be deployed to production.

=> DevOps team will use user permissions for git repo

   - Read permissions
   - Read and write permissions

Note : Before production deployment we need to go for code freeze. (15 days or 1 month before.)


=====================================================================================================================
                                            17-OCT-24
                                     =========================

1) Maven : build tool

2) Git hub and bitbucket : version control siftwares



================
Webservers
================

=> server is a software used to run our web application.

=> Users can access our web application by sending request to server.

=> Users will use client software to send request to server.

  Ex : browser (google chrome, firefox, edge)

=> Server is responsible to handle user requests and responses.

=> We have several servers in the market to run our web applications.

1) Tomcat
2) JBoss
3) glassfish
4) WebLogic
5) WebSphere
6) ISS ( .net)

Note : To run web application, server is mandatory.

=> The process of executing web application using server is called as deployment.

=============================
What is build & Deployment
=============================

   Build = compile + Test + Package

   Deployment = executing in server

===================
Tomcat server
===================

=> Tomcat is free and open source software.

=> Tomcat is a web server developed by Apache organization.

=> Tomcat server developed using Java language

Note : to run tomcat server, java must be installed.

=> Tomcat server is used to run java based web applications.

=> Tomcat server supports multiple operating syetems.

=> Tomcat server runs on 8080 port number (we can change it)

========================
Tomcat setup in Linux
========================

=> create Linux VM using Amazon Linux AMI in AWS cloud.

=> Connect to Linux VM using mobaxterm

=> Install mavan software.

  Ex : sudo yum install maven

Note : when we install maven, Java software installed automatically.

  mvn -version
  java -version

=> We can download tomcat software form its official website.

 URL : 	https://tomcat.apache.org/download-90.cgi

=> Download tomcat server tar file.

  Ex: wget <url>

=> Extract zip  file
 $ unzip <zip file name>

=> Go to tomcat directory and see directory structure.

  cd <tomcat-dir>

  ll

====================================
Tomcat server directory structure
====================================

1) bin : It contains files to start & stop server (windows : bat || Linux : sh)

   Windows : startup.bat & shutdown.bat

   Linux : startup.sh & shutdown.sh

2) conf : It contains tomcat server configuration files.

    ex : server.xml, tomcat-user.xml

3) webapps : Its called  as deployment folder. We will keep WAR file here for execution.

4) lib  : Its contains libraries required for server (.jar files)

5) temp : Temporary files will be created here. (we can delete them)

6) Logs : Server log messages will be stored here.


============================
Web app deployment process
============================
step 1 : Create maven web application in ec2-user home directory.

mvn archetype:generate -DgroupId=com.ashokit -DartifactId=my-web-app -DarchetypeArtifactId=maven-archetype-webapp -DarchetypeGroupId=org.apache.maven.archetypes -DinteractiveMode=false

cd <project directory>

ls -l

step 2: build project using maven goal

mvn clean package

ls -l

ls -l target

cd ..

step 3 : copy application war file into tomcat server webapps folder for execution

 Ex : cp my-web-app/target/my-web-app.war apache-tomcat-9.0.107/webapps/my-web-app.war

cd <app-war-file-path> <tomcat-webapp-dir-path>

step 4 : start tomcat server from bin directory.

cd apache-dir/bin

ls -l

chmod 777 Catalina.sh

chmod 777 startup.sh

Note : enable tomcat server port number 8080 in ec2 VM security group inbound rules.

step 5: Access our web application using browser.

URL : https://public-ip:8080/my-web-app


===============================================
How to change the tomcat server port number ?
===============================================

=> Tomcat server default port is 8080

=> We can change this port number using "server.xml" file

  File location:  tomcat-dir/conf/server.xml

=> After changing port number, stop and start the tomcat server.

=> Enable new port number in ec2 VM security group  inbound rules.

    server URL : https://public-ip/port-num

=================================================================================================================================================================================
                                                                                    18-OCT-24
                                                                         =========================
Re-deploying the application
===================================================================================================================

step 1:
---

ls -l
    Jul  2 07:01 apache-tomcat-9.0.107
    Jul  2 07:17 apache-tomcat-9.0.107.zip
    Jul 10 18:35 my-web-app

step 2:
---

 cd my-web-app  --->  cd src/main/webapp  -----> ls -l ------->

 9 16:20 WEB-INF
9 16:20 index.jsp

edit index.jsp


drwxr-xr-x. 9 ec2-user ec2-user    16384 Jul  2 07:01 apache-tomcat-9.0.107
-rw-r--r--. 1 ec2-user ec2-user 13552562 Jul  2 07:17 apache-tomcat-9.0.107.zip
drwxr-xr-x. 5 ec2-user ec2-user       58 Jul  9 16:22 my-web-app
[ec2-user@ip-172-31-12-66 ~]$ cd my-web-app
[ec2-user@ip-172-31-12-66 my-web-app]$ ls -l

step 3: re build the app
---

-rw-r--r--. 1 ec2-user ec2-user 2202 Jul  9 16:20 pom.xml
drwxr-xr-x. 3 ec2-user ec2-user   18 Jul  9 16:20 src
drwxr-xr-x. 4 ec2-user ec2-user   68 Jul  9 16:22 target
[ec2-user@ip-172-31-12-66 my-web-app]$ mvn clean package

step 4: deploy the app in tomcat server
---

[ec2-user@ip-172-31-12-66 ~]$  cp my-web-app/target/my-web-app.war apache-tomcat-9.0.107/webapps/my-web-app.war


step 5: got to tomcat bin folder and stop and start the server
---

[ec2-user@ip-172-31-12-66 bin]$ sh shutdown.sh


[ec2-user@ip-172-31-12-66 bin]$ sh startup.sh

==================================================================================================================================


Enable tomcat Admin console access
====================================

=> Bydefault tomcat server admin console is accessible in the same machine in which tomcat server is running.

Note : If you wish to modify this restriction, you will need to host managers context.xml file.

=> File location : <tomcat-folder> / webapp/manager/META-INF / context.xml

=> In manager context file. change value <Value> section like below (allow attribute value changed)

<Context antiresourcelocking ="false" provilaged="true"> 
   <Value classname=" org.apache.catalina.values.RemoteAddrvalue" Allow =".\\\*">
<context> 


============================================================================
Add tomcat user in "<tomcat>/ conf/tomcat-users.xml" 	file like below
============================================================================


<role rolename="manager-gui"/>
<role rolename="manager-script"/>
<role rolename="admin-gui"/>

<user username="tomcat" password="tomcat" roles="manager-gui"/>
<user username="admin" password="admin" roles="manager-gui,admin-gui,manager-script"/>

=> Once above changes completed then start tomcat server and access tomcat admin console in browser.

  server URL : <public-ip> : <port number>


=======================================
Tomcat-summery
=======================================

1) What is tomcat server architecture

2) What is tomcat

3) Tomcat setup in Linux

4) Tomcat server directory server

5) War file deployement

6) Enable tomcat admin console

7) Changing tomcat server port number.


 
=================================================================================================================================================================================
                                                                                    21-OCT-24
                                                                         =========================
==================
Sonar Qube
==================

=> Its used for code quality checking

=> Its called as code review software.

=> Using code review we can identify developers mistake in the code.

Note : it will help improve code quality.

Note : If code quality gate is passed then only we will deploy that code. otherwise we should not deploy that code into server.

=> SonarQube supports 30+ languages code review.

  Ex : c , c++ java, .net, SQL

=> We can use SonarQube in 2 ways

    1) Community edition (free of cost)

    2) Enterprise edition (Licensed)



                                                                ################# Not fully practiced ##############################



=================================================================================================================================================================================
                                                                                    **17-OCT-24**

                                                                         **=========================**

Docker : containerization

       - What is containerization and why
       - Docker Intro
       - Docker Architecture
       - Docker setup in Linux machine
       - Docker Image
       - Docker Container
       - Docker registry
       - Docker file
       - Docker network
       - Docker volume
       - Docker compose
       - Docker swarm (Orchestration)

2.  Kubernetes (k8s) : Orchestration (management)

 
       - What is Orchestration and why
       - Kubernetes Intro
       - Kubernetes Architecture
       - k8s cluster setup (AWS EKS cluster)
       - Pods
       - Services (ClusterIP, NodePort, LBR)
       - Name spaces
       - Replica Set
       - Deployment
       - Config Map & Secrets
       - IngressController
       - HPA
       - Helm chats
       - Volumes
       - Liveness & Readyness probes
       - Grafana & promethues


3) Jenkins : CI CD server

       - What is CI CD & why
       - Jenkins Introduction
       - Jenkins setup in Linux VM
       - Jenkins plugins
       - Jenkins system configuration
       - Jenkins Jobs (Free style)
       - Jenkins pipeline
              - Declarative
              - Scripted (Groovy)

       - Jenkins Master & Slave Architecture
       - Jenkins Email integration
       - User Management in Jenkins (RBAC)


4) Project setup by integrating Docker + Kubernetes + Jenkins

=========================================================================


Application architecture
==========================

1) Front end : User Interface

2) Backend : Business logic

3) database : Storage

======================================
Tech stack of application
=======================================

Front end : Angular

Backend : java 17

Database: MySQL DB server 8.5

Webserver : Tomcat 9.0


Note ; If we want to run our application code, then we need to setup all required dependencies in the machine.


Note : Dependencies nothing but software's required to run our application

   Ex : java 17, angular 16, MySQL 8.5, Tomcat Server 9.0

Note : If we want to run our application in 100 machines then it is hectic task to setup dependencies and there is a chance of human mistake.

=> To overcome above problem we will use Docker tool.


==========================
What is Docker
=========================

=> Docker is an open source platform for developing, shipping and running applications in containers.

=> Containers are light-weight isolates environment that package application and their dependencies.

Benefits of using Docker : Portability, scalability, consistency, and resource efficiency

=> Docker is a free and open source software.

=> Docker is used for containerization

=> With help of Docker, we can run our application in any machine.

=> Docker will take care of dependencies installation required for application execution.

=> We will make our application portable using Docker.

Note : Docker is platform independent. We can use docker in Windows, Linux and mac also


Docker container : Application code + Application dependencies.


Note : containerization means packaging application code + application dependencies as a single unit for execution.



=================================================================================================================================================================================
                                                                                    18-OCT-24
                                                                           =========================

Docker Architecture
==========================

Docker File
2.  Docker Image
3.  Docker Registry
4.  Docker container

=> Docker file is used to specify where is app code and what dependencies are required for our application execution.

Note : Using Docker file we will build docker image.

=> Docker image is package which contains app code and app dependencies.

  Docker Image = App code + app dependencies

=> Docker registry is used to store docker images.

Note : When we run docker image, then docker container will be created. Docker container is a Linux VM

=> Inside docker container our application will be executed.


=====================================
Install Docker in Linux Machine
=====================================

step 1: Create ec2 VM (amazon Linux) & connect with the machine using SSH client

step 2: execute below commands

# Install Docker
sudo yum update -y
sudo yum install docker -y
sudo service docker start


# add ec2-user user to docker group
sudo usermod -aG docker ec2-user


# Exit from terminal and connect again
exit

# verify docker installation
docker -v


===================================
Docker Commands
=================================

docker images : to display docker images available in our system

docker pull <image id/ name> to download docker image from docker hub

   Ex : docker pull ashokit/spring-boot-rest-api

docker rmi <image-id/ name> : to delete docker image

docker run : <image id/ name> : to create/ run docker container

docker ps : do display running docker containers

docker ps -a : to display running + stopped containers

docker stop <container-id> : to stop docker running container

docker start <container-id> : to start docker container which is in stopped state.

docker rm <container-id> : to delete docker container

# To delete stopped containers + unused images + build cashe
docker system prune -a :


=============================================================
Running real-world applications using docker image
============================================================

docker pull ashokit/spring-boot-rest-api
docker run  ashokit/spring-boot-rest-api
docker run -d ashokit/spring-boot-rest-api
docker ps
docker logs <container-id>
docker run -d -p <host-port>: <container-port> imageid
$docker run -d -p 9090:9090 fcb4841c55d1

app url : https://public-ip:host-port/welcome/{name}


=================================================================================================================================================================================
                                                                                    19-OCT-24
                                                                           =========================
docker pull ashokit/python-flask-app
docker run -d ashokit/python-flask-app
docker run -d -p 5000:5000 <image-id>

# To build dokcer image
$ docker build -t <tag-name> .

# To login into docker hub account
$ docker login

# To push docker image into docker hub
$ docker push <image-name>


python app url : https://public-ip:host-port

Note : Here -d represents detached mode
 
       Here -p represents port mapping (host port : container port)

Note : host port and container port no need to be same



====================
Docker file
====================

=> Docker file contains set of instructions to build docker image

file name : Dockerfile

Note we will keep dokerfile inside project directory.


=================================================================================================================================================================================
                                                                                    21-OCT-24
                                                                           =========================

=> To write docker file we will use below commands

1) FROM
2) MAINTAINER
3) RUN
4) CMD
5) COPY
6) ADD
7) WORKDIR
8) EXPOSE
9) ENTRYPOINT
10) USER


=========
FROM
==========

=> Its used to specify base image to create our Docker image

 	Ex : FROM OpenJDK : 17

             FROM python : 3.3

              FROM Node : 19

              FROM MySQL : 8.5

===============
MAINTAINER
==============

=> To specify author of Docker file

  Ex: MAINTAINER Ashokashok@gmail.com

  Note : its optional


===============
RUN
==============

=> RUN keyword is used to specify instructions (commands) which are required to execute at the time of Docker image creation

Ex : RUN 'git clone <repo-url>'

     RUN 'mvn clean package'

Note : We can specify multiple RUN instructions in Docker file and all those will execute in sequential manner.


===============
CMD
==============

=> CMD keyword is used to specify instructions (commands) which are required to execute at the time of Docker container creation

 Ex :  CMD  "java -jar <jar-file-name>"

     CMD " python app.py"

Note : If we write multiple CMD instructions in docker file docker will execute only last CMD instruction.


========
COPY
========

=> Copy instruction is used to copy the file from source to destination

Note : Its used to copy application code  from host machine to container machine.

   Source : HOST machine
   Destination : Container machine.

Ex: COPY target/app.jar  /user/app/

    COPY terget/webapp.war  /user/app/

    COPY app.py   /user/app/

=========
ADD
=========

=> ADD instruction is used to copy the files from source to destination.

   Ex:
   ADD target/app.jar  /user/app/
   ADD <file-url> /user/app/


=============
WORKDIR
=============
 
=> WORKDIR instruction is used to 	set/ change working directory in container machine.

  Ex:
          COPY target app.jar /	/user/app
           WORKDIR /user/app

CMD "java -jar app.jar"

============
EXPOSE
============

=> EXPOSE instruction used to specify application running on which port number

Ex : 8080


============
ENTRYPOINT
============

=> Its used to execute instructions when container is getting created.

Note : ENTRYPOINT is used as alternate for CMD instructions.

CMD  "java -jar app.jar"

ENTRYPOINT ["java", "jar", "app.jar"]

====================================================
What the difference between CMD and ENTRYPOINT
=====================================================

CMD instructions we can override

ENTRYPOINT instructions we cannot override


============
USER
============

=> Its used to set user account to execute dockerfile commands

USER 'ashokit'

RUN echo 'hi'

==============================================
Dockerizing Spring boot application
=============================================

=> Spring boot is a java framework to develop java based application.

=> Spring boot application is packaged as jar file
 
   Ex; java -jar app.jar

Note : When we run spring boot application jar file, internally spring boot will use tomcat server as "embedded container" with default port 8080.

============================================Java Spring boot app Dockerfile=====================================

FROM OpenJDK:17
MIANTAINER "Ashok"
COPY target/sb-app.jar /user/app/
WORKDIR /user/app/
EXPOSE 8080
ENTRYPOINT ["java", "-jar", "sb-app.jar"]



=================================================================================================================================================================================
                                                                                    22-OCT-24
                                                                           =========================


=====================================================
Dockerizing Java web application (No Spring boot)
=====================================================


=> Normal java web apps will be packaged as WAR fil.

Note : WAR file will be created in side project target directory.

=> To execute that java web application we need to deploy that war file tomcat server

=> Inside tomcat server we will have "webapps" folder. Its called as deployment folder.

=> TO run war file, we need to keep war file in tomcat/webapp folder.

==================================Docker file for Java Web application======================================

FROM tomcat:latest
MAINTAINER "ashok<6253632>"
EXPOSE 8080
COPY target/maven-web-app.war /usr/local/tomcat/webapps/


===============================================================================================================
Java Web App
====================

1) Clone git repo

https://github.com/ashokitschool/maven-web-app.git

2) Go inside project directory and perform maven build

cd maven-web-app
mvn clean package

3) Create docker image

 docker build -t ashokit/maven-web-app

 docker images

4) create docker container

$ docker run -d -p 8080:8080 ashokit/maven-web-app

5) Access application  URL

 for windows : localhost:8080/maven-web-app
   Linux : publicIP:8080/maven-web-app

============================================================================

Java Spring boot app
=======================

1) Clone git repo

  https://github.com/ashokitschool/spring-boot-docker-app.git

2) Go inside project directory and perform maven build

 cd spring-boot-docker-app

mvn clean package

3)  Create docker image

docker build -t ashokit/sb-app

docker images

4) Create docker container

docker run -d -p 8080:8080 ashokit/sb-app
docker ps
docker logs <container-id>

5) Access application URL in browser

    wondows : localhost:8080

   Linux : publicIP:8080

==============================================
Dockerizing python application
==============================================

=> Python is a general purpose language

=> Its also called scripting language

=> We dont need any build  tool for python application

=> We can run python application code directly like below

     ex: python app.py

=> If we need any libraries for python application development then we will mention them in "requirements.txt" file

Note : we will use "python pip " s/w to download libraries configured in requirements.txt file



=====================Python Flask app docker file====================

FROM python :3.6
COPY ./usr/app/
WORKDIR /usr/app

EXPOSE 5000
RUN pip install -r requirements.txt
ENTRYPOINT ["python", "app.py"]
===================================================================================

process working with python app
==================================

1) Clone git repo

  https://github.com/ashokitschool/python-flask-docker-app.git

2) go to project directory
 
  	$ cd python-flask-docker-app


 3) Create docker image

   $ docker build -t py-app .

4) Create and run the container

  $ docker run -d -p 5000:5000 py-app
5) Check
 
   docker ps

    docker logs <container-id>

6) Access application
 
 localhost: 5000


======================================
How to access docker container
====================================

# Display docker container which are running mode.
  docker ps

# go inside container from linux machine
 docker exec -it <container-id> /bin/bash/

# go inside docker container from windows host
docker exec -it <container-id> sh


 
=============================
Assignment for today
==============================

1) Setup Jenkins server as docker container

2) setup MySQL 	DB as docker container

3) Dockerizing Angular and React Application




=================================================================================================================================================================================
                                                                                    23-OCT-24
                                                                           =========================

1) Docker network

2) Docker volumes

3) Docker compose

4) Docker Swarm



====================
Docker network
=======================

=> Network is all about communication

=> Docker network is used to provide isolated network for containers

=> If we run 2 containers under same network, then our containers can communicate with each other

=> By default we have 3 types of networks in Docker.

   1) Bridge

   2) Host

   3) none

=> Bridge network containers are used to run stand alone containers. It will assign one IP for containers. Its the default network for the containers.

=> Host network is also used to run stand alone containers. This will not assign any IP for our containers.

=> None means no network will be available.

# display docker networks
$ docker network ls

# create docker networks
$ docker network create ashokit-nw

# inspect docker networks
$ docker network inspect ashokit-nw

# run docker container with custom  networks
$ docker run -d -p 8080:8080 --network ashokit-nw ashokit/maven-web-app


# to delete custom network
$ docker network rm ashokit-nw


====================
Docker compose
===================

=> Earlier people developed projects using Monolithic applications (everything in single app)

           Ex : MakeMyTrip ---------> One docker image ----------------> One container

=> Now a days projects are developed based on micro services architecture

=> Micro services means multiple backend APIs will be available

 Ex :
      1) Hostel-api
      2) flights-api
      3) trains-api
      4) cabs api

=> for every API we need to create separate container

Note :: When we have multiple containers like this management will become very difficult (create, start, stop)

=> To overcome this problems we will use docker compose.

=> DOcker compose is used to manage multi -container based applications

=> In docker-compose using single command we can create / stop/ start multiple containers at a time.

===================================
What is Docker-compose.yml file ?
===================================

=> Docker-compose.yml file used to specify container information

=> The default file name is Docker-compose.yml (we can change it)

=> Docker-compose.yml file contains below 4 sections

   version : it represents compose yml version
   services : it represents containers info(image-name , port mapping)
   network : it represents network to run our containers
   volumes : Represents containers storage location

======================
Docker compose setup
=====================

# install docker compose
sudo curl -L

# check docker compose is installed or no
$ docker-compose --version


======================================================
Spring boot with MySQL DB using docker compose
======================================================
=> I want to deploy spring boot application with MySQL db

   a) MYSQL db will run on one container
   b) Spring boot app will run on one container

Note : Spring boot application should connect with MySQL DB. That means app container will depends on DB container.

=> As we need container communication those 2 containers should run on a same network.

=> below is the docker-compose.yml to manage spring boot app with MySQL DB deployment.

---
version :
services :
network :
...



####################################### check out the video again/ Not fully completed #############################





=================================================================================================================================================================================
                                                                                    24-OCT-24
                                                                           =========================

=================
Docker Swarm
=================

=> Docker swarm is an orchestration  flatform

=> orchestration means managing the processes (containers)

=> Using Docker swarm will setup Docker cluster.

Note : Cluster means group of servers.



================================
Docker Swarm cluster setup
================================

=> Create 3 ec2 instances (ubuntu) & install docker in all 3 instances using below 2 commands.

 curl -fsSL https://get.docker.com -o get-docker.sh

sudo sh get-docker.sh

1 -Master Node

2 - Worker Node


Note : Enable 2377 port in security group for swarm cluster communication.

=> Connect to master machine and execute below commands.

# Initialize docker swarm cluster.
$ sudo docker swarm init --advertise-addr <private-Ip of master node>

   Ex : sudo docker swarm init --advertise-addr 172.31.33.114

# Get join token form master (this token is used by worker to join with master)
sudo docker swarm join-token worker .

Note : Copy the token and execute in all worker node with sudo permissions.

Ex : sudo docker swarm join --token

  docker swarm join --token SWMTKN-1-2so6pzgrmpqy4x2prhzou6v2370sezgolggtefimt6fzkkqesn-c91195j6ag3pa4onve03wxo5i 172.31.33.114:2377

Note : with above steps docker swarm cluster is ready.

=> In docker swarm we need to deploy our application is service.

#To create and run the containers
$sudo docker service create --name maven-web-app -p 8080:8080 ashokit/mavenwebapp


#To increate the count of the conainers
$ sudo docker service scale maven-web-app=3

# To list the containers runing
$ sudo docker service ps maven-web-app







=================================================================================================================================================================================
                                                                                    25-OCT-24
                                                                           =========================

=========================================
Deploy Angular application using Docker
=========================================

=> Angular is a framework which is used to develop front-end of an application (UI)

=> We will use NODE software to build and run angular application.

=> Angular app libraries will be configured in "package.json" file.

=> To download libraries configured we will use "npm install" command

=> To build Angular application we will use below command.

    npm run build --prod

Note : when we run above command 'dist' folder will be generated for deployement.

=> To deploy Angular application we will use NGINX webserver.


======================================== Docker file for Angular app ================================================

FROM node:18 as build
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build --prod


FROM nginx:alpine
COPY --from=build /app/dist/<your-project-name> /user/share/nginx/html

EXPOSE 80

====================================================================================

Deploy angular application using Docker
===============================================


## step 1 : Clone git repo
git clone https://github.com/ashokitschool/angular_docker_app.git


## step 2 : Go inside project directory and build docker image
cd angular_docker_app
docker build -t ng-app .

## step 3 : run docker container
docker images
docker run -d -p 80:80 ng-app

## step 4 : Access angular app in browser
 windows : localhost:80
 linux : public-ip:80
 


====================================================================================

Deploy React application using Docker
===============================================

## step 1 : Clone git repo
git clone https://github.com/ashokitschool/ReactJS_Docker_App.git

## step 2 : Go inside project directory and build docker image
cd ReactJS_Docker_App
docker build -t react-app .

## step 3 : run docker container
docker images
docker run -d -p 80:80 react-app

## step 4 : Access react app in browser
 windows : localhost:80
 linux : public-ip:80
 

=============================================
Build + deploy Spring boot app using Docker
==============================================

FROM maven : latest as build
WORKDIR /app
RUN git clone <repo-url>
RUN mvn clean package
FROM OpenJDK:18-jdk-alpine
WORKDIR /app
COPY --from=build /app/target/app.jar .
cd ReactJS_Docker_App
EXPOSE 8080
ENTRYPOINT ["java", "-jar", "app.jar"]



=================================================================================================================================================================================
                                                                                    28-OCT-24
                                                                           =========================

###################
KUBERNETES
###################

=> Its a free and open source software.

=> Google developed this K8S using "Go" programming language.

=> K8S provides Orchestration (Management) platform.

=> K8S used to manage containers.

    Ex : create, stop, start, delete, scale up, scale down.

=> Kubernetes provides a framework for managing the complex task of deploying, scaling and operating applications in containers.

================
Advantages
===============

Container Orchestration : manages containers
Self healing : If any container got crashed then it will replace with new container.
Load balancing : Load will be distributed among all containers which are up and running.
Auto scaling : Based on demand, containers will be increased or decreased.


============================
Docker VS Kubernetes
============================

Docker : containerization platform

Note : Packaging our application code and dependencies as single unit for execution is called as containerization.

Kubernetes : Orchestration platform.

Note : Orchestration means managing the containers.


=================================
Kubernetes Architecture
================================

=> K8S will follow cluster architecture

=> Cluster means group of servers will be available.

=> In k8S cluster we will have master node (control plane) and worker nodes.



================================
Kubernetes Cluster Components
================================

Control Node (Master Node)
 
        - API Server
        - Scheduler
        - Controller Manager
        - ETCD

2) Worker Nodes

        - Kubelet
        - Kube Proxy
        - Docker Runtime
        - POD
        - Container

=> To display our application using K8s then we need to communicate with control nodes.

=> We will use KUBECTL (CLI) to communicate with control place

=> "API server" will receive the request given by "kubeCTL" and it will store that request in "ETCD" with pending status.

=> "ETCD" in an internal database of K8s cluster.

=> "scheduler " will identify pending requests available in "ETCD" and it will identify worker node to schedule that task.

Note : Scheduler will identify worker nodes using kubelet

=> Kubernetes is called as Node Agent. It will maintain all worker node information.

=> "Kube proxy" will provide network for the cluster communication.

=> "Controller Manager" will verify all the tasks are working as expected or no.

=> In worker node, "Docker Engine" will available to run the docker containers.

=> In K8s containers will be created as POD.

=> POD is a smallest building block that we  can create in k8s cluster to run our application.

Note : In K8s, everything will be represented as PODs only.

=> DODs are used to deploy applications and manage their life cycle within a Kubernetes environment.


========================
K8S cluster setup
========================

=> We can setup K8s cluster in multiple ways.

Self managed k8s cluster
 
   a) Mini Kube => Single Node cluster => Only for beginners

   b) Kubeadm  => Multi node cluster  => we need to setup verything

2. Provider Managed K8S cluster (readymade)
 
   a) AWS EKS (Elastic Kubernetes Cluster)
   b) Azure AKS
   c) GCP GKE  (Google Kubernetes Engine)


Note : Provided manages clusters and chargable


=========================================================
Assignment : Setup minikube software in Linux VM
=========================================================




===============================
Kubernetes Resources
==============================

PODS
Services
 
     - ClusterIP
     - Nodeport
     - Loadbalancer

3) namespaces

4) ReplicaSet

5) Deployement

6) DaemonSet

7) StatefulSet

8) PV & PVC

8) INgresController

9) HPA

10) Helmcharts

11) Metric server

12) ConfigMap & Secret

13) Libeness & Readyness Probes

14) Taints & Tolerations




=================================================================================================================================================================================
                                                                                    29-OCT-24
                                                                          =========================
 


=========================
Minikube Setup
========================


GitHub URL : https://github.com/ashokitschool/DevOps-Documents/blob/main/13_MiniKube_Setup.md


## Step-1 : Setup Linux VM

1) Login into AWS Cloud account
2) Create Linux VM with Ubuntu AMI (t2.medium or t3.medium)
3) Select Storage as 50 GB (Default is 8 GB only for Linux)
2) Create Linux VM and connect to it using SSH Client

## Step-2 : Install Docker In Ubuntu VM

```



sudo apt update



curl -fsSL get.docker.com | /bin/bash



sudo usermod -aG docker ubuntu 



exit



```
## Step-3 : Updating system packages and installing Minikube dependencies

```



sudo apt update



sudo apt install -y curl wget apt-transport-https







```

## Step-4 : Installing Minikube

```



curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64







sudo install minikube-linux-amd64 /usr/local/bin/minikube







minikube version



```

## Step-5 : Install Kubectl (Kubernetes Client)

```



curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl



chmod +x kubectl



sudo mv kubectl /usr/local/bin/



kubectl version -o yaml



```

## Step-6 : Start MiniKube Server

```



minikube start — driver=docker



```

## Step-7 : Check MiniKube status

```



minikube status



```

## Step-8 : Access K8S Cluster

```



kubectl cluster-info



```

## Step-9 : Access K8S Nodes

```



kubectl get nodes



```


kubectl get services
```







===============================================================







What is POD ?



===================







=> Pod is smallest building block in the k8s cluster 







=> Applications will be deployed as pods only in the k8s cluster.







=> We can create multiple cluster for one single application.







=> To create a pod we will use manifest yml file.







=> In POD manifest YML we will Configure Docker Image







=> If any pod is damaged then Kubernetes will replace that with new pod (self healing)







=> If application running in multiple pods, then k8s will distribute load to all running pods (load balancing).







=> PODS count can be increased and decreased based on the load. K8s will take care of scalability.











=========================



K8S Services



==========================







=> K8S services is used to expose PODs







=> We have 3 types of services in  K8s 







\&nbsp;  - ClusterIp



\&nbsp;  - NodePort



\&nbsp;  - Loadbalancer















======================



What is ClusterIP ?



======================







=> When we deploy app in K8S, then pods will be created and every pod will have IP address.







=> POD is a short lived object







=> When is pod is crashed/damaged k8s will replace that with new pod.







=> When pod is re-created then new IP will be assigned.







Note : Its not recommended to access POD using POD Ip.







=> "Cluster IP" services is used to link all the pods to single IP.







Note : Cluster Ip service provides static IP to access PODS.







=> Cluster IP service we can access with in the cluster only.















===============================



What is NodePort service ?



==============================







=> NodePort service is used to expose our pods outside of the cluster.







=> Using NodePort service we can access our application pods with node Public Ip address.







Note : When we use Node public Ip to access out pods then all requests will go to single node only and burden will be increased on that node.







=> If we want to distribute the load to all pods which are running in all worker nodes, then we have to expose our pods using "loadbalancer" service.











==========================



What is loadbalancer ?



==============================



\&nbsp;



=> Its used to expose our pods outside of the cluster using Load balancer.







=> When we access "Load Balancer" URL It will distribute the load to all pods which are running in all worker nodes.















=====================================



How to create PODS in K8S ?



=====================================







=> To create PODS we will use YML (manfiest file)







---



apiVersion: <version-number>







kind: <k8s-resource-type>







metadata: <name>







spec: <container-info>



...











\\# Execute manifiest file yml



$ kubectl apply -f <manifest-yml>











===============================================================================



Assignment : Deploy Java web app as pod in k8s cluster \\\& access in browser



===============================================================================







---



apiVersion: v1



kind: Pod



metadata: 



\&nbsp;name: javawebapppod



\&nbsp;labels: 



\&nbsp; app: javawebapp



spec: 



\&nbsp;container: 



\&nbsp;- name: javawebappcontainer



\&nbsp;  image: ashokit/javawebapp



\&nbsp;  ports: 



\&nbsp;  - containerPort: 8080



...











\\# Check pods available



$ kubectl get pods







\\# Execute pod manifest yml



$ kubectl apply -f <file-name>







\\# Describe Pod



$ kubectl describe pod <pod-name>

===========================
K8S service manifest YML
============================


=> Using service we can expose pods outside of cluster
---

apiVersion: v1



kind: Service



metadata: 



\&nbsp;name: javawebappsvc



spec: 



\&nbsp;type: nodePort 



\&nbsp;selector: 



\&nbsp; app: javawebapp







ports: 



\&nbsp;- port: 80



\&nbsp;  targetPort: 8080



\&nbsp;  nodePort: 30070







...











\\# Check k8s service



$ kubectl get svc







\\# execute service yml



$ kubectl apply -f <file-name>







\\# to delete all the resources in k8s cluster



$ kubectl delete all --all















Note : Once service got created we need to enable Node-Port number in security group inbound rules.







\&nbsp;   URL to access app : https://node-public-ip:node-port/ java-web-app/



===========================	



What is NodePort number ? 



===========================







=> If we dont specify node port in service manifest yml then k8s will assign random node port number for this service in the range of 30000 to 32767



===========================



K8S namespaces



===========================







=> Namespaces are used to group k8s resources







\&nbsp;   database-pods ====> create under one namespace



\&nbsp;



\&nbsp;   backend-app-pods  ===> create under one namespace







\&nbsp;   front-end-pods ===> create under one namespace







=> We can create multiple namespaces in k8s cluster.







\&nbsp;Ex: ashokit-a1-ns, ashokit-s2-ns







=> Each namespace is isolated with another namespace 







\##### 



Note : When we delete a namespace, all the resources belongs to that namespace will be deleted.







\\# Display all namespaces available



$ kubectl get ns







\\# Get pods of default namespaces



$ kubectl get pods







\\# get the pods available in kube system namespaces



$ kubectl get pods -n kube-system.







\&nbsp;=> in k8s we can create namespaces in 2 ways







\&nbsp;  1) using kubectl create ns command







\&nbsp;  2) using manifest yml











\\## Approach-1 :







\&nbsp;$ kubectl create ns ashokit-ns







\\## Approach 2 : 







---



apiVersion: v1



kind: Namespace



metadata: 



\&nbsp;name: ashokit-ns-2



...











$ kubectl apply -f <namespace-name>











\\# Delete namespace



$ kubectl delete ns <namespace-name>







==================================



Namespace + Pods + Service



==================================







---



apiVersion: v1



kind: Namespace



metadata:



\&nbsp;name: ashokit-ns



---







apiVersion: v1



kind: Pod



metadata:



 name: javawebapppod



\&nbsp;namespace: ashokit-ns



 labels:



  app: javawebapp



spec:



 container:



 - name: javawebappcontainer



   image: ashokit/javawebapp



   ports:



   - containerPort: 8080



...







apiVersion: v1



kind: Service



metadata:



 name: javawebappsvc



 namespace: ashokit-ns



spec:



 type: nodePort



 selector:



  app: javawebapp







ports:



 - port: 80



   targetPort: 8080



   nodePort: 30070







...







$ kubectl apply -f <yml>







$ kubectl get ns







$ kubectl get pods -n ashokit-ns







$ kubectl get service -n ashokit-ns 







$ kubectl get all -n ashokit-ns







$ minikube service javawebappsvc -n ashokit-ns







\\# to stop minikube cluster



$ minikube stop 











===========================================================================







1\. What is orchestration

2\. K8S introduction

3\. K8S advantages

4\. K8S architecture

5\. K8S architecture Components

6\. K8S cluster setup

7\. K8S resources

8\. What is POD

9\. What is service (ClusterID, NodePort, LBR)

10\. What is Namespace











=================================================================================================================================================================================



                                                                                    04-NOV-24



                                                                          =========================







=> If we create PODs directly then K8S will not manage POD life cycle







=> If Pod is crashed/ deleted then pod will not get recreated 







=> If we want to get the benefit of k8s self-healing/ load-balancing/ auto-scaling, then we should create pod using K8S resources.











=========================



K8S Resources



==========================







=> K8S resources are used to manage pod lifecycle







1\. ReplicationController (RC) outdated

2\. ReplicaSet

3\. Deployement

4\. DeamonSet

5\. StatefulSet







=> RC will support only one label as selector to identify pods







Ex : selector: 



\&nbsp;       matchLales: 



\&nbsp;           app: javawebapp











=========================



ReplicaSet



=======================







=> Its one of the K8Sresources which is used to create and manage pods



=> ReplicaSet will take care of POD lifecycle



Note : When pod is crashed/damaged then replicaSet will not create new pod



=> Always it will maintain given no.of pods count available for our application



Ex: replicas: 2



=> With this approach we can achieve high availability for our application.



=> By using RS we can scale up and scale down our pods.







--------------------



manifest yml ..



........................



Note : When we execute above command 	replicaset will check how many pods are currently running based on what it will decide scale up or scale down.



Note : If we want to delete the pods then we have to delete the resources which created those pods.



$ kubectl delete rs javawebrs



Note : IN replicaSet scale-up and scale-down is manual process.



===================================



Different between RC and RS ?



====================================



=> RC will support one label as selector to identify pods



Ex: 



selector: 



\&nbsp;  app: javawebapp



=> RS will support multiple Match lables as selectors to identify pods



Ex : selector:



\&nbsp;      matchLables: 



\&nbsp;        app: javawebapp







=================================================================================================================================================================================



                                                                                    06-NOV-24



                                                                          =========================



======================

K8S deployment

======================



=> Its one of the K8S resources/ component



=> Its most recommended approach to deploy our application in K8S cluster. 



=> Deployment will manage pod life cycle.



=> We have below advantages with deployement



&nbsp; 1) Zero downtime



&nbsp; 2) Auto scaling



&nbsp; 3) Rolling update \& Rollback



=> We have below strategies for deployment.



&nbsp; 1) ReCreate



&nbsp; 2) Rolling update



=> ReCreate means it will delete all the existing pods and will create new pods



Note: When we use ReCreate strategy we will have application downtimne.



=> Rollong update means it will delete old pods and create new pods one by one.



---

apiVersion: apps/v1

kind: Deployment

metadata: 

&nbsp;name: javawebdeploy

spec: 

&nbsp;replicas: 2

&nbsp;strategy: 

&nbsp; type:  ReCreate

&nbsp;selector: 

&nbsp; matchLabels: 

&nbsp;  app: javawebapp

&nbsp;template:

&nbsp; metadata: 

&nbsp;   name: javawebpod

&nbsp;   labels: 

&nbsp;    app: javawebapp

&nbsp; spec: 

&nbsp;  container: 

&nbsp;  - name: javawebappcontainer

&nbsp;    image: ashokit/javawebapp

&nbsp;     ports: 

&nbsp;     - containerPort: 8080

---

apiVersion: v1

kind: Service

metadata: 

&nbsp;name: javawebsvc

spec: 

&nbsp;type: NodePort

&nbsp;selector: 

&nbsp; app: javawebapp

&nbsp;ports: 

&nbsp; - port: 80

&nbsp;   targetport: 8080

&nbsp;   nodePort: 300709

...







==============

Auto Scaling

================



=> Its the process of increasing or decreasing the infrastructure resources based on demand.



=> Auto scaling has 2 types



&nbsp; 1) Horizontal scaling

&nbsp; 2) Vertical scaling



=> Horizontal scaling means increasing/decreasing number of instances/servers/pods.



=> Vertical scaling means increasing/decreasing capacity of system. (CPU. RAM etc)





=====================================

Horizontal scale autoscalar (HPA)

===================================



=> HPA is used to scale up/ scale down no.of pods replicas based on the demand



=> To scale up/ scale down the pods, HPA needs metrics from cluster nodes



Note : To get metrics from the nodes we will use "Metric server". We need to deploy metric server POD in worker nodes.



=> Metric server is an application that collects metrics from objects such as PODs and nodes according to the state of CPU, RAM and keep them in time. 



=> HPA will interact with Metrics Server to get worker nodes metrics and based on that information HPA will take decisions for scale up or scale down.





\## Step 1: Install metri API



\## Step 2: Deploy sample application on pods



\## step 3: Create Services



\## step 4: Create HPA



\## step 5: Increase the load



\## step 6: Observe HPA behavior and HPA events





Youtube link : https://youtu.be/c-tsJrcB50I?si=IBeznqJ9WhW7HnvQ



git hub : https://github.com/ashokitschool/k8s\_metrics\_server.git



HPA git repo : https://github.com/ashokitschool/kubernetes\_manifest\_yml\_files.git









=================================================================================================================================================================================

                                                                                    07-NOV-24

                                                                          =========================



================

AWS EKS

================



=> EKS stands for Elastic Kubernetes Service



=> EKS is fully manages service in AWS cloud



=> EKS provides ready made control plane for us



=> EKS is most robust platform to run our K8S control plane



Note : EKS is a paid service





============================

AWS EKS setup

==================



\# Step - 1 : Create EKS Management Host in AWS #



1\) Launch new Ubuntu VM using AWS Ec2 ( t2.micro )	  

2\) Connect to machine and install kubectl using below commands  

```
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --short --client
```

3\) Install AWS CLI latest version using below commands 

```
sudo apt install unzip
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version
```



4\) Install eksctl using below commands

```
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
eksctl version
```

\# Step - 2 : Create IAM role \& attach to EKS Management Host #



1\) Create New Role using IAM service ( Select Usecase - ec2 ) 	

2\) Add below permissions for the role <br/>

&nbsp;	- Administrator - acces <br/>

&nbsp;		

3\) Enter Role Name (eksroleec2) 

4\) Attach created role to EKS Management Host (Select EC2 => Click on Security => Modify IAM Role => attach IAM role we have created) 



\# Step - 3 : Create EKS Cluster using eksctl # 

\*\*Syntax:\*\* 



eksctl create cluster --name cluster-name  \\

--region region-name \\

--node-type instance-type \\

--nodes-min 2 \\

--nodes-max 2 \\ 

--zones <AZ-1>,<AZ-2>



\## N. Virgina: <br/>

`

eksctl create cluster --name ashokit-cluster4 --region us-east-1 --node-type t2.medium  --zones us-east-1a,us-east-1b

`	

\## Mumbai: <br/>

`

eksctl create cluster --name ashokit-cluster4 --region ap-south-1 --node-type t2.medium  --zones ap-south-1a,ap-south-1b

`



\## Note: Cluster creation will take 5 to 10 mins of time (we have to wait). After cluster created we can check nodes using below command.



`

&nbsp;kubectl get nodes  

`



\# Note: We should be able to see EKS cluster nodes here.\*\*



\# We are done with our Setup #

&nbsp;	

\# Step - 4 : After your practise, delete Cluster and other resources we have used in AWS Cloud to avoid billing #



```
eksctl delete cluster --name ashokit-cluster4 --region ap-south-1
```



=================================



git hub : https://github.com/ashokitschool/DevOps-Documents/blob/main/05-EKS-Setup.md



====================================



=================================================================================================================================================================================

                                                                                    08-NOV-24

                                                                          =========================



Revision

=====================



What is Orchestration

Docker VS K8S

Kubernetes Intro

K8S advantages

K8S architecture

Minikube setup in Linux and Windows

AWS EKS setup

PODS

Services (ClusterIP, NopePort, LoadBalancer)

Namespaces

K8S resources

ReplicationController (RC)

ReplicaSet (RS)

Deployment 

HPA with metrics API





====================

ConfigMap \& Secrets

=======================



=> For every application we will have multiple environments in the real time.



DEV

SIT

UAT

PILOT

PROD





=> DEV env used by developers to perform code integration testing.



=> SIT env used by testing team to perform "System integration Testing"



=> UAT env is used by client to perform "User Accepting Testing"



=> PILOT env is used for pre-production testing



=> PROD is used  for application live environment



Note : Every environment will have some properties to run the application



Database props

SMPT props

Kafka props

Redis props

Payment gateway props





\## We should not configure the above properties in application If so , then our application will  have hardcoded properties to support for only one environment deployment.



=> If we want to deploy our application in another env then we have to change properties and package our application `then create docker image then deploy in k8s cluster. Its time taking process and error prone.



Note : To make our application loosely coupled with env  we should not configure the properties in the application. 





=> To make our application loosely coupled with env properties we can use ConfigMap \& Secrets.



=> ConfigMap \& Secret will allow you to de-couple application properties from docker images so that our application can be deployed in any env.



Note : 	At the time of application deployment we can supply environment properties to the application using configmap \& Secret.



=> ConfigMap and Secret will store data in key-value format.



=> To store non-sensitive information we will use configmap and to store sensitive information we will use secret.





=================================

ConfigMap manifest YML



=====================================

---

apiVersion: v1

kind: ConfigMap

metadata:

&nbsp; name: sample-config

&nbsp; namespace: default

data:

&nbsp; database\_url: "mongodb://localhost:27017"

&nbsp; database\_name: "exampledb"



&nbsp;  smtp\_host: ""

&nbsp;  smtp\_port: ""

...





$ kubectl get configmap



$ kubectl apply -f <configmap-yml-file>





=========================

Secret manifest yml

========================



apiVersion: v1

kind: Secret

metadata:

&nbsp; name: my-secret

&nbsp; namespace: default

type: Opaque

data:

&nbsp; username: YWRtaW4=      # base64 for "admin"

&nbsp; password: c2VjcmV0cGFzcw==  # base64 for "secretpass"







$ kubectl get secret 



$ kubectl apply -f <secret-manifest-yml>





====================================

Reading data from configMap

=====================================



valueFrom:

 configMapKeyRef:

   name: example-configmap

   key: database\_url



valueForm:

 secretKeyRef:

   name: secret-dev

   key: password





=============

Assignment:  

===============



=> Deploy MySQL Db in k8s cluster by using configMap \& Secret.



=> To do this task we will create below manifest yml files



mysql-config.yml

mysql-secret.yml

mysql-deployment.yml

mysql-service.yml





Note : Paste this in chatGPT.it will give you complete process with  files.



$ kubectl delete all --all





=================================================================================================================================================================================

                                                                                    09-NOV-24

                                                                                  =============

Assignment:



=> Deploy MySQL Db in k8s cluster by using configMap \& Secret.



=> To do this task we will create below manifest yml files



mysql-config.yml

mysql-secret.yml

mysql-deployment.yml

mysql-service.yml





Note : Paste this in chatGPT.it will give you complete process with  files.



$ kubectl delete all --all                                                                     





==================

HELM Charts

===================



yum

apt

rpm 





=================================================================================================================================================================================

                                                                                    11-NOV-24

                                                                                  =============



premethios 



Grafana





ELK /	

EFK STack 

Splunk 







=================================================================================================================================================================================

                                                                                    13-NOV-24

                                                                                  =============



========================

Blue Green Deployment

========================



Complete setup and worked deployment and deployed new code and directing the traffic to new release. 











=================================================================================================================================================================================

                                                                                    14-NOV-24

                                                                                  =============	



===========================

Ingress Controller

==========================



=> Its Kubernetes resources and its used to map external traffic to internal services running in the k8s cluster.



=> It supports path based routing



&nbsp;  path = /hoteks ===> farward that request to "hotels-service"



&nbsp;  path = /flights ===> farward that request to "flights-service"





Youtube : https://youtu.be/Fa7f0cHgx2k









===================

K8S Summery

===================





1\) K8S introduction

2\) K8S Architecture

3\) K8S cluster setup

&nbsp;  - Minikube

&nbsp;  - AWS EKS 

4\) K8S terminology

5\) PODS

6\) services 

7\) Namespaces

8\) Replication controller

9\) Replica set 

10\) Deployment

11\) HPA

12\) Blue green deployment

13\) ConfigMap \& Secret

14\) MySQL DB deployment in k8s

15\) DeamonSet

16\) StatefulSet

17\) EFK stack

18\) HELM Charts

19\) Ingress Controller





===========Pending topics =================



20\) PV \& PVC

21\) Taints \& Tolerations

22\) Liveness \& Readyness

23\) Grafana Prometheus





============================================================

Working with Jenkins , setup and creating  a basic job

===========================================================











=================================================================================================================================================================================

                                                                                    15-NOV-24

                                                                                  =============	





==============

Jenkins

==============



=> Its a free and open source s/w



=> Jenkins developed by using java language



=> Jenkins is used fot CI CD operations



=> CI CD is used to automate project build \& deployment process.



===============================

What is Build and Deployment

================================



=> Takes latest source code from git repo

===============

Jenkins JOB

===============



Git + maven + Jenkins + tomcat 



When code change in repo wwe can just see the changes in the browser


=================================================================================================================================================================================

                                                                                    18-NOV-24

                                                                                  =============	
==================

User Management

===================



=> In the project multiple people will be available\\

&nbsp;

&nbsp;  1) Development

&nbsp;  2) Testing

&nbsp;  3) Operations 



Note : For every team member Jenkins logins access will be provided. 



=> Operations team : Responsible to create/edit/delete Jenkins jobs



=> Development/Testing : Responsible only to run the jobs



=> By default everybody can access all the permissions


=============================

Master \& Slave architecture

=============================



=> If we use single machine to run the Jenkins Job then burden will be increased in that machine.



=> If the burden is increased then machine will be crash



=> To reduce burden Jenkins we will use master slave architecture configuration.



=> Using master and slave architecture we can distribute the load.



=====================

Jenkins master

=====================



=> The machine which contains Jenkins s/w is called master machine.



=> Its used to create and schedule the jobs



=> Jenkins master is responsible to distribute the load to slave machines.

=====================

Jenkins slave

=====================



=> It will receive the task from master to execute the jobs.
=================================================================================================================================================================================

                                                                                    19-NOV-24

                                                                                  =============	
working on setup creating master and slave machines



====================================================


&nbsp;	

====================================

Jenkins Pipeline

======================================



=> Jenkins pipeline is a way to define CI CD process as a code



=> When we are dealing with CI CD process then pipelines are highly recommended. 



=> Jenkins pipeline contains set of stages



&nbsp;  stage-1 : Clone git repo

&nbsp;

&nbsp;  stage-2 : Maven built

&nbsp;

&nbsp;  stage-3 : Code review

&nbsp;

&nbsp;  stage-4 : Artifact upload

&nbsp;

&nbsp;  stage-5 : Docker image



&nbsp;  stage-6 : Push Image

&nbsp;

&nbsp;  stage-7 : K8S deployment





=> We can write Jenkins pipeline in 2 ways 



&nbsp;  1) Declarative pipeline



&nbsp;  2) Scripted Pipeline  (Goovy)







===============================

Jenkins Declarative pipeline

===============================



sample

====================================



pipeline {

&nbsp;   agent any



&nbsp;  



&nbsp;   stages {

&nbsp;       stage('stage-1') {

&nbsp;           steps {

&nbsp;               echo 'Hello World'

&nbsp;           }

&nbsp;       }

&nbsp;        stage('stage-2') {

&nbsp;           steps {

&nbsp;               echo 'Prasenting Brand New Audi A8'

&nbsp;           }

&nbsp;       }

&nbsp;   }





====================================================

Jenkins Pipeline with git + maven

==================================================





pipeline {

&nbsp;   agent any

&nbsp;   

&nbsp;   tools {

&nbsp;       maven   "maven-3.9.11"

&nbsp;   }

&nbsp;   stages {

&nbsp;       stage('Git Clone') {

&nbsp;           steps {

&nbsp;               git 'https://github.com/ashokitschool/maven-web-app.git'

&nbsp;           }

&nbsp;       }

&nbsp;        stage('Maven Build') {

&nbsp;           steps {

&nbsp;              sh 'mvn clean package'

&nbsp;           }

&nbsp;       }

&nbsp;   }

}



======================================================================

Assignment : Jenkins pipeline with Git + Maven + Tomcat

=====================================================================

=================================================================================================================================================================================

                                                                                    20-NOV-24

                                                                                  =============


Assignment : Jenkins pipeline with Git + Maven + Tomcat

==============================================================



=> To deploy war into tomcat server using CI CD pipeline we will use "SSH-agent" in pipeline



=> SSH agent is used to establish remote ssh connection from one Linux vm to another Linux vm 



=> Using SSH agent Jenkins server will copy war file into tomcat server. 



=> Install SSH-Agent plugin into Jenkins



&nbsp;      => Manage Jenkins



&nbsp;      => Select Plugins

&nbsp;

&nbsp;      => Check available plugins 



&nbsp;      => Search for SSH-agent and install it 





============================================

How to configure SSH-agent in pipeline

============================================



=> Go to Jenkins pipeline syntax option 



=> Select SSH Agent



=> Add tomcat server credentials with user name and private key



=> Generate SSH agent code block 





&nbsp; sshagent (\['tomcat-server-credentials']){

&nbsp;

&nbsp;  // logic ..



}



=> Inside ssh agent code block we will use 'scp' commands 



sh 'scp -o StringHostKeyChecking=no <src> <destination>'



=> src is where war file available 



=> Destination is tomcat server webapp folder



=> Below is the Jenkins pipeline with 3 stages





===============================================================================================================

pipeline {

&nbsp;   agent any



&nbsp;   tools {

&nbsp;       maven 'maven-3.9.11' // Make sure this version is defined in Jenkins Global Tool Config

&nbsp;   }



&nbsp;   stages {

&nbsp;       stage('Git Clone') {

&nbsp;           steps {

&nbsp;               git 'https://github.com/ashokitschool/maven-web-app.git'

&nbsp;           }

&nbsp;       }



&nbsp;       stage('Maven Build') {

&nbsp;           steps {

&nbsp;               sh 'mvn clean package'

&nbsp;           }

&nbsp;       }



&nbsp;       stage('Deployment') {

&nbsp;           steps {

&nbsp;               sshagent(\['tomcat-credentials']) {

&nbsp;                   sh '''

&nbsp;                       scp -o StrictHostKeyChecking=no target/maven-web-app.war ec2-user@3.85.29.142:/home/ec2-user/apache-tomcat-9.0.107/webapps/

&nbsp;                   '''

&nbsp;               }

&nbsp;           }

&nbsp;       }

&nbsp;   }

}





=======================================================================================================================



===================================

Email Notification in Jenkins

===================================



=> We can configure email notification in Jenkins



=> After Jenkins job execution we can trigger email to project team members regarding job execution status.



=> To send emails from Jenkins we need to configure SMTP properties 



&nbsp; SMTP : Simple Email Transfer Protocol 



Note : I real-time we can use company provided SMTP properties 



&nbsp;       but here we can use gmail SMPT properties for practice



SMTP host : smtp.gmail.com

SMTP port : 587



username : your email id

password : gmail-account-app-pwd (not login password)





=============================================

How to configure SMTP properties in Jenkins

=============================================



=> Go to manage Jenkins



=> Go to system 



=> Go to "extended email notification"



=> Add SMTP details like server and port



=> add authentication  with gmail and app-pwd



=> Select TLS checkbox



=> Below is the sample pipeline to send email notification as post build action 





=============some code ===================





======================

Jenkins Summery

=====================



=> Build and deployment



=> Challenges in manual build and deployment



=> Automated build and deployment



=> CI \& CD



=> Jenkins Introduction



=> Jenkins setup ii Linux



=> Jenkins Job creation



=> Jenkins build trigger options



=> User management in Jenkins (RBAC)



=> Git + Maven + Tomcat + Jenkins



=> Master and slave configuration\\



=> Jenkins CI CD pipeline



=> Declarative pipeline creation \\



=> Git + maven + tomcat + Jenkins integration with pipeline



=> ssh agent configuration 



=> Email notification in Jenkins
=> Shared libraries in Jenkins

=> Parallel stages execution in Jenkins

=> Scripted pipeline

=> Multi-branch pipeline


=================================================================================================================================================================================

                                                                                    22-NOV-24

                                                                                  ============
=============================================================

How to work with parallel stages in Jenkins pipeline

============================================================



=> Jenkins job contains multiple jobs like below and all the stages will be executed sequentially.



&nbsp;	

========================practice pending ===============================


==========================================

What is shared library in Jenkins ? 

==========================================



=> Shared library means collection of groovy scripting files that we  will use in Jenkins Pipeline.



=> Shared library is used for groovy scripting re-usability



=> When we have multiple Jenkins pipelines then some common logic will be available in multiple pipelines like below.



&nbsp;	  Hotels-api ===> Jenkins pipeline



&nbsp;         Flights-api ===> Jenkins pipeline



&nbsp;        Trains-api ====> Jenkins pipeline

=> Instead writing common logic in every pipeline we can create shared library and we can re-use it 

Notes : Shared library we can store in git repo 

========================================

What is shared library in Jenkins

=================================

step-1 : create shared library and store it in git repo

&nbsp; Git repo url : https://github.com/ashokitschool/my\_shared\_libraries.git

step-2: Configure shared library in Jenkins

&nbsp; => manage Jenkins f=> Configure system

&nbsp; => Global trusted pipeline Libraries 

&nbsp; => Click add to create new Library 

&nbsp; => Give unique name for library



step-3: Create Jenkins pipeline and use above created shared libraries 

============================

Jenkins Scripted pipeline

=============================


=> Scripted pipelines are used to define CI CD workflow using groovy scripting.

=> Scripted pipeline will provide more flexibility and more control on pipeline stages execution.

=> We can implement error handling logic in programmatic way using scripted pipelines.

==================================

scripted pipeline syntax

===============================

node {




stage('git clone')

{
echo 'git cloning..'
}



stage('maven build')

{
echo 'maven build process..'
}

}

=============================================================================================

Assignment : 1 :git + maven + tomcat integration in scripted pipeline



&nbsp;            2 : How to take Jenkins backup and restore



&nbsp;  ref video : from ashokit channel

=========================================================================================


=================================================================================================================================================================================

                                                                                    25-NOV-24

                                                                                  =============

**==================================**

**What is multi branch pipeline ?**

**==================================**

=> Git repo is used for source code integration

=> In git repo we will have multiple branches

&nbsp;a) main

&nbsp;b) develop

&nbsp;c) feature

&nbsp;d) release

=> Jenkins pipeline taking code from particular branch and perform build and deployment process.

=> When I execute Jenkins pipeline code it has to build the code which is available in all the branches of given git repository.  Then we can use multi branch pipeline.

=============================

Role Management in Jenkins

=================================



=> We can access to users by using roles concept.

Role based matrix strategy plugin

plugin name : Role based authorization strategy

=> Create role

=> Assign permission for the role

=> Add users to the role


=================================================================================================================================================================================

                                                                                    26-NOV-24

                                                                                  ============
=> Working practice with multi branch pipeline


============

Sonar
=================================================================================================================================================================================

                                                                                    27-NOV-24

                                                                                  ============
===============

SonarQube

==============

=> Code quality checking software or Code review software

=> Using SonarQube we can perform code review to identify developers mistake in the code.

=> SonarQube software developed by using Java language

=> SonarQube supporting 30+ languages code review 

================

Sonar Issues

=================

=> SonarQube server will identify below types of issues in the project



&nbsp; 	a) Bugs (danger, it may stop code execution)

&nbsp; 

&nbsp;       b) Vulnerabilities (Security hotpots)


&nbsp;       c) Code smell (not danger, but week design)

&nbsp;       d) Duplicate code blocks

&nbsp;       e) Code coverage (how many lines of code is tested)

Note : DevOps team is responsible to perform code review and send code review report to the development team.



Note : Development team is responsible to fix the issues identified by the Sonar server as part of code review process.


=========================

Sonar Quality Profiles

========================

=> Set of rules to perform code reviews.

=> In SonarQube for every language one quality profile available

&nbsp; Java project ==> Java Quality Profile  ==> Java Rules

&nbsp; Python Project ==> Python Quality Profile ==> Python Rules

&nbsp; PHP Project ==> PHP Quality Profile ==> Python Rules

========================

Sonar Quality Gate

========================
=> Quality Gate represents overall project code quality is passed or failed.



Note : If project quality gate is failed then we should not deploy that code.

====================

SonarQube setup

==================



steps to setup : https://github.com/ashokitschool/DevOps-Documents/blob/main/06-Sonar-Setup-Docker.md



Note : Sonar Server runs on port number 9000. Enable it in ec2 vm security group inbound rules.



default username : admin

default password : admin


Sanity testing 

==========================

Sonar server setup in CI CD

=============================

=> To integrate Sonar Server in Jenkins pipeline we need sonar server token.

=> Steps to generate a token

&nbsp;  Goto profiles => my account => Security => Generate Token

Token :  sqa\_f0ea874e93f93894a1fc55296cd56df3923bb5a5

Note : In Jenkins pipeline after maven build process completed we will perform code review using SonarQube server.

Note : We can user sonar trial version for practice

=> In company we will use sonar enterprise version (commercial)

=======================

Artifactory Server

======================

=======================================

Sonartype Nexux : Artifactory server

============================================



=> Nexux is an open source software (OSS) Its free of cost



=> Its sued as antifactory repository server



=> Its used to build project build artifacts.



&nbsp;Ex : jar or war files



=> Project artifacts will be stored in antifactory server for backup purpose

=> Nexus software is developed by using java language

=> The current version of Nexus is 3.x version

Note : The alternate for Nexus is JFrog.

================================================================

Q) Whats the difference between GitHub and Nexus

===============================================================

=> Git hub is user as Source code repository server. Its used to store project source code.

=> Nexus is used as anrtifactory repository server. Its used to store project build artifacts. (jar and war)
=====================

Nexus Server Setup

=====================

Setup steps : https://github.com/ashokitschool/DevOps-Documents/blob/main/07-Nexus-Setup-Docker.md



Note : Nexus server will run on port number 8081. Enable it in EC2 vm security inbound rules.

========================

Nexus Repository Type

=======================
=> In Nexus we can create 2 types of repositories 



1. Snapshot repository
2. Release Repository


=> If project is under development then that project artifacts will be uploaded into snap shot repository.

=> If project is completed then that project artifacts will be uploaded into release repository.

Release Repo URL : 
================================

Nexus Integration in CI CD

==============================

=> We can integrate Nexus repository artifact upload process as part of Jenkins CI CD pipeline.

Note : Once maven build process completed then we can upload project artifacts into Nexus repository.

========================

DevOps Project - 1 ;

============================

=> Create CI CD pipelines using below devOps tools

1. Jenkins
2. Git hub
3. Maven 
4. SonarQube
5. Nexus 
6. Tomcat



=> Project Setup process : https://github.com/ashokitschool/DevOps-Documents/blob/main/09-DEVOPS-PROJECT-SETUP.md

Video in : ashoit youtube channel

=================================================================================================================================================================================

                                                                                    02-DEC-24

                                                                                  =============
Project -2 : working with Docker Kubernetes and Jenkins.


Then do practice


=================================================================================================================================================================================

                                                                                    03-DEC-24

                                                                                  =============



Resume building
======================

some ecommerce projects
==============================

1) www.nykaa.com
2)  www.milton.com
3)  www.decathlon.in
4)  www.godrejinterio.com
5)  www.bantia.com



=================================================================================================================================================================================

                                                                                    04-DEC-24

                                                                                  =============

self introduction 
==================

4 parts
---------------

part -1 : your details : name, exp, role , company  name

part -2 : Technical Skills

part -3 : Your project

part -4 : Roles and responsibilities

=> Hi, I am Ashok having total3.5 years of experience as a DevOps engineer with aws cloud platform and currently I am working as DevOps engineer with XYZ company.

=> Coming to my technical skills, I am having hands on experience with several DevOps tools like terraform, ansible, git , sonar, nexus, docker, k8s, Jenkins. eks, etc..

=> I am having experience with AWS cloud services like EC2, RDS, Iam , VPC, S3, EKS , CloudWatch, sns etc.

=> I am having good experience with Linux VMs

=> Coming to y project, I am working with health insurance system which is providing health and insurance plans to Rhode Island state citizens in USA.

Front end : angular
backend : Java Spring boot microservices 
Database : Oracle

=> Coming to my roles and responsibilities

1) Create infrastructure in AWS Cloud using Terraform scripts

2) Configuration management using ansible playbooks

3) Managing source code repositories in bitbucket

4) User Management bitbucket

5) Managing application environment (DEV, SIT, UAT, PROD)

6) Managing Docker files and Images

7) Creating k8s manifest files

8) CI CD pipelines creation and monitoring

9) Code quality checking

10) Monitoring and managing application logs


============================================
Day to day activities of a DevOps engineer
============================================

=> First thing we need to check our email official inbox

=> If required we need to reply to those email 

=> Check meeting schedules for the day and get ready for those meetings

=> Check Jira dashboards and work on tasks which are assigned on your name.

Note : If task is not assigned then check pending tasks and assign pending tasks to your name start working on that. If no pending task not available then inform your lead/ scrum master / manager regarding that.


=> join daily scrum calls and give work updates to scrum master.

- working on which story 

- when it will be completed 

- any challenges to complete it 

- any help required 

- any blockers

Note : If you are not able to join scrum call for today then send work updated to scrum master in email.

	
===========================================================
What kind of tasks will be assigned to DevOps engineer ?
=============================================================

1) Infrastructure creation

	Ex : 	Setup EKS Cluster, Setup SonarQube server, Setup Jenkins server.
2) write ansible playbooks

3) Setup new environment (DEV/ SIT/ UAT/ PROD) (server setup)

4) Source code repo creation 

5) Manage users and users permissions in repo

6) Setup CI CD pipelines for build and deployment

7) Monitoring application (Grafana, premethios, ELK)

8) POC (Proof of Concept) (experiment)

9) Documentation

10) Provide KT sessions

11) Conduct internal training sessions



=================================================================================================================================================================================

                                                                                    06-DEC-24

                                                                                  =============

================================
3 tier architecture project
================================

=> Application contains 3 layers 

   1) Database

   2) backend app

   3) Front end app


=> Database is used to store data permanently

=> Backend app contains business logic

  Ex : Amount debit, amount credit, send OTP , send email, validate credentials.. etc..

=> Front end contains presentation logic  (User Interfaces)



Step -1 : Setup MySQL DB in AWS cloud using RDS

Note : Once RDS instance is created note down below details

  DB Endpoint : 
  DB Username:
  Db password: 
  Db initial name : ashokit_ecomm

Step -2 : Connect with MySQL using MySQL workbench and execute SQL queries to load data into database tables.

Note : DB queries files available in backend_api git repo

backend api git repo : https://github.com/ashokitschool/01_products_api.git

DB queries file name : DB_Setup.sql

Step -3: Configure database properties in backend application

step-4 : Deploy backend application using Jenkins CI CD pipelines with below devOps tools

 a) Git hub
 b) Maven
 c) SonarQube
 c) Docker
 d) K8S deployment



step -5 : Access backend API url and test it working or no

step -6 : Configure backend app URL in front end application

step-7 ; Create Jenkins pipeline to deploy front end application using below tools

   1) Git hub 
   2) Docker
   3) k8S

step 8: Access front end application and test it 	


to complete deployment of 3 tier application 
---------------------------------------------

youtube video : https://youtu.be/6cX8l22CKCI

f

=================================================================================================================================================================================

                                                                                    17-DEC-24

                                                                                  =============


Self introductions................





=================================================================================================================================================================================

                                                                                    19-DEC-24

                                                                                  =============


ashok mock interview....




=================================================================================================================================================================================

                                                                                    20-DEC-24

                                                                                  =============	

=======================
What is HELM ?
======================

=> In Linux we will use package managers to install a softwere.

Ex : yum, apt, 	rpm etc..

=> HELM is a package manager which is used to install required software's in k8S cluster

=> HEL will use charts to install required packages.

=> Charts means collection of configuration files (manifest yml)

=> Using HELM charts we can install promethuesa and grafana server.



===========================
HELM installation
=============================

$ curl -fsSl -o get_heml.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3

$ chmod 700 get_helm.sh

$ ./get_helm.sh

$ helm



=> Check do we have metric server on the cluster

$ kubectl top pods
$ kubectl top nodes

#check helm repos
$helm repo ls

# Add the  metric server repo to helm 
$ helm repo add metrics-server metrics-server/metrics-server 

# kubectl top pods

# kubectl top nodes


=================================
install Prometheus and Grafana 
=====================================

steps ...


everything about Prometheus , Grafana .. working with all the real time 



=================================================================================================================================================================================

                                                                                    23-DEC-24

                                                                                  =============	

1) Node Selector in K8s 
2) Node Affinity
3) Taints
4) Tolerations

===================
Node Selector
================

=> Node selector is used to schedule the pods on particular worker node only.

=> To achieve this we can assign label for the worker node and we can configure that worker node label in our manifest yml as node selector.

Note : If node selector is matching with worker node label then our pods will be created on that particular worker node. If node selector  not matching with worker node label then pods will not be scheduled for execution.

=> Execute below manifest yml to create nginx deployment with 3 pod replicas

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      nodeSelector:
        name: ashokit-wn-1
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

...

$ kubectl apply -f <yml>

$ kubectl get pods

Note : Here pods will be in pending state because no worker node label is matching with node selector 	configured in manifest yml file.

# Configure label for worker node
$ kubectl get pods
kubectl edit node <node-name>

# configure below label under label section  
name : ashotit-wn-1

$ kubectl get pods

Note : here pods will come to running state.

==================================================================================================================



===================
Node Affinity
================

=> Node Affinity works based on proffered approach. If node selector is matching with any worker node label then schedule pods on that worker node only. If match is not found then schedule pods on any available worker node in the cluster.


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: name
            operator: In
            values:
            - ashokit
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80

...


$ kubectl apply -f <yml>

$ kubectl get pods -o wide 


==============
Taints
==============

=> Taints are used to 	make worker nodes not available for pod scheduling.

=> We have 3 popular tains

1) No schedule

2) No Execute

3) prefer no schedule 

# Create taint for worker node 
$ kubectl taint nodes <node-name> key1=value1:NoSchedule

# remove taint from worker node 
$ kubectl taint nodes <node-name> key1=value1:NoSchedule-


====================
Tolerations
=====================



====================================================================================================================================================================================

                                                                                    25-DEC-24

                                                                                  =============	

===============
PV $ PVC
================

PVC : persistence Volume Claim

PV : persistence Volume


=> PV is sued to request storage for the pods

=> PVC will request for PV using storage classes

=> Storage classes will provide different types of storages for PV 

  Ex: efs, ebs, standard etc..

=> PV provides storage for the resources in k8s 

Note : PV and PVC will use with the stateful set


========================================================================
NOte : Difference between Deployment vs Statefulset vs DeamonSet
============================================================================

================================
Liveness and Readyness Probes
===============================

Liveness Probes : Its used to determine POD is still running or no. If POD is not running then k8s will restart that pod based on restart policy.

Readyness Probe : Its used to determine POD is ready to serve request or no. If POD is not ready to serve the request then k8s will remove that pod from the load balancer.


-------------------sample docker file -----------------
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
...




====================================================================================================================================================================================

                                                                                    03-Jav-25
                                                                                  =============	

======================================================
Complete end to end project hands on experience 
======================================================

========================
3 tier project setup
=======================

=> Application contains 3 layers

 1) database 
 2) Backend
 3) Frontend


===============
DB setup 
==============

Step-1 : Setup AWS RDS MuSQL instance and note down db details

DB Endpoint : 
DB Username : admin
DB password : admin4321
DB initial name ; ashokit_ecomm

step-2 : Connect with my SQL DB with workbench








































